{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words_set = set(text)\n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    word_number = 1\n",
    "    for word in words_set:\n",
    "        vocab_to_int[word] = word_number\n",
    "        int_to_vocab[word_number] = word\n",
    "        word_number = word_number + 1\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"    \n",
    "    tokens_dict = {'.': 'Period', \n",
    "                   ',': 'Comma',                    \n",
    "                   ';': 'Semicolon',\n",
    "                   '\"': 'Quotation_Mark',\n",
    "                   '!': 'Exclamation_mark',\n",
    "                   '?': 'Question_Mark',\n",
    "                   '(': 'Left_Parentheses',\n",
    "                   ')': 'Right_Parentheses',\n",
    "                   '--': 'Dash',\n",
    "                   '\\n': 'Return'}   \n",
    "  \n",
    "    return tokens_dict\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/dlearning/lib/python3.5/site-packages/ipykernel_launcher.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"    \n",
    "    Input = tf.placeholder(tf.int32, name=\"input\", shape = [None,None])\n",
    "    Targets = tf.placeholder(tf.int32, shape = [None,None])\n",
    "    Learning_rate = tf.placeholder(tf.float32)\n",
    "    return Input, Targets, Learning_rate\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"    \n",
    "    lstm_layers = 2\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_layers, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm] * rnn_size) \n",
    "    state = tf.identity(cell.zero_state(batch_size, tf.float32),name='initial_state')    \n",
    "    return cell, state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)\n",
    "#get_init_cell(10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1)) \n",
    "    embed = tf.nn.embedding_lookup(embedding,input_data)\n",
    "    return embed\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"    \n",
    "    rnn_outputs,final_state = tf.nn.dynamic_rnn(cell,inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state,name=\"final_state\")\n",
    "    \n",
    "    return rnn_outputs, final_state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"    \n",
    "    embed = get_embed(input_data,vocab_size,embed_dim)    \n",
    "    outputs, final_state = build_rnn(cell,embed)    \n",
    "    logits = tf.contrib.layers.fully_connected(outputs,vocab_size,activation_fn=None)\n",
    "    return logits, final_state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    total_words = len(int_text)\n",
    "    total_batches = total_words // (batch_size*seq_length)\n",
    "    \n",
    "    inputs_start = 0;\n",
    "    inputs_end = total_batches*batch_size*seq_length\n",
    "    inputs_array = np.array(int_text[inputs_start:inputs_end])\n",
    "    inputs = np.split(np.array(int_text[inputs_start:inputs_end]).reshape(batch_size, -1), total_batches, 1)\n",
    "    \n",
    "    targets_start = inputs_start + 1\n",
    "    targets_end = inputs_end + 1\n",
    "    targets = np.split(np.array(int_text[targets_start:targets_end]).reshape(batch_size, -1), total_batches, 1)\n",
    "   \n",
    "    # set the last value of targets to the first value in inputs\n",
    "    targets[total_batches-1][batch_size-1][seq_length-1] = inputs[0][0][0]    \n",
    "    \n",
    "    final_batch = np.array(list(zip(inputs, targets))).reshape(total_batches, 2, batch_size, seq_length)\n",
    "\n",
    "    return final_batch\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "#get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)\n",
    "\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 3\n",
    "# RNN Size\n",
    "rnn_size = 1\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 1\n",
    "# Sequence Length\n",
    "seq_length = 2\n",
    "# Learning Rate\n",
    "learning_rate = 0.1\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/11516   train_loss = 8.822\n",
      "Epoch   0 Batch    1/11516   train_loss = 8.822\n",
      "Epoch   0 Batch    2/11516   train_loss = 8.821\n",
      "Epoch   0 Batch    3/11516   train_loss = 8.823\n",
      "Epoch   0 Batch    4/11516   train_loss = 8.558\n",
      "Epoch   0 Batch    5/11516   train_loss = 8.574\n",
      "Epoch   0 Batch    6/11516   train_loss = 8.581\n",
      "Epoch   0 Batch    7/11516   train_loss = 8.420\n",
      "Epoch   0 Batch    8/11516   train_loss = 8.061\n",
      "Epoch   0 Batch    9/11516   train_loss = 8.210\n",
      "Epoch   0 Batch   10/11516   train_loss = 8.247\n",
      "Epoch   0 Batch   11/11516   train_loss = 7.920\n",
      "Epoch   0 Batch   12/11516   train_loss = 7.358\n",
      "Epoch   0 Batch   13/11516   train_loss = 7.195\n",
      "Epoch   0 Batch   14/11516   train_loss = 6.722\n",
      "Epoch   0 Batch   15/11516   train_loss = 6.834\n",
      "Epoch   0 Batch   16/11516   train_loss = 7.942\n",
      "Epoch   0 Batch   17/11516   train_loss = 6.980\n",
      "Epoch   0 Batch   18/11516   train_loss = 5.954\n",
      "Epoch   0 Batch   19/11516   train_loss = 6.512\n",
      "Epoch   0 Batch   20/11516   train_loss = 9.647\n",
      "Epoch   0 Batch   21/11516   train_loss = 5.692\n",
      "Epoch   0 Batch   22/11516   train_loss = 9.087\n",
      "Epoch   0 Batch   23/11516   train_loss = 6.503\n",
      "Epoch   0 Batch   24/11516   train_loss = 8.340\n",
      "Epoch   0 Batch   25/11516   train_loss = 9.197\n",
      "Epoch   0 Batch   26/11516   train_loss = 7.815\n",
      "Epoch   0 Batch   27/11516   train_loss = 7.348\n",
      "Epoch   0 Batch   28/11516   train_loss = 7.476\n",
      "Epoch   0 Batch   29/11516   train_loss = 9.191\n",
      "Epoch   0 Batch   30/11516   train_loss = 5.140\n",
      "Epoch   0 Batch   31/11516   train_loss = 5.590\n",
      "Epoch   0 Batch   32/11516   train_loss = 8.359\n",
      "Epoch   0 Batch   33/11516   train_loss = 5.546\n",
      "Epoch   0 Batch   34/11516   train_loss = 6.787\n",
      "Epoch   0 Batch   35/11516   train_loss = 8.009\n",
      "Epoch   0 Batch   36/11516   train_loss = 5.715\n",
      "Epoch   0 Batch   37/11516   train_loss = 9.605\n",
      "Epoch   0 Batch   38/11516   train_loss = 8.056\n",
      "Epoch   0 Batch   39/11516   train_loss = 6.987\n",
      "Epoch   0 Batch   40/11516   train_loss = 8.140\n",
      "Epoch   0 Batch   41/11516   train_loss = 8.959\n",
      "Epoch   0 Batch   42/11516   train_loss = 4.288\n",
      "Epoch   0 Batch   43/11516   train_loss = 6.829\n",
      "Epoch   0 Batch   44/11516   train_loss = 7.826\n",
      "Epoch   0 Batch   45/11516   train_loss = 8.059\n",
      "Epoch   0 Batch   46/11516   train_loss = 7.745\n",
      "Epoch   0 Batch   47/11516   train_loss = 9.061\n",
      "Epoch   0 Batch   48/11516   train_loss = 5.866\n",
      "Epoch   0 Batch   49/11516   train_loss = 7.795\n",
      "Epoch   0 Batch   50/11516   train_loss = 7.242\n",
      "Epoch   0 Batch   51/11516   train_loss = 8.136\n",
      "Epoch   0 Batch   52/11516   train_loss = 7.857\n",
      "Epoch   0 Batch   53/11516   train_loss = 7.392\n",
      "Epoch   0 Batch   54/11516   train_loss = 7.035\n",
      "Epoch   0 Batch   55/11516   train_loss = 6.350\n",
      "Epoch   0 Batch   56/11516   train_loss = 6.829\n",
      "Epoch   0 Batch   57/11516   train_loss = 7.328\n",
      "Epoch   0 Batch   58/11516   train_loss = 5.583\n",
      "Epoch   0 Batch   59/11516   train_loss = 5.654\n",
      "Epoch   0 Batch   60/11516   train_loss = 7.685\n",
      "Epoch   0 Batch   61/11516   train_loss = 8.440\n",
      "Epoch   0 Batch   62/11516   train_loss = 6.360\n",
      "Epoch   0 Batch   63/11516   train_loss = 5.508\n",
      "Epoch   0 Batch   64/11516   train_loss = 7.257\n",
      "Epoch   0 Batch   65/11516   train_loss = 5.251\n",
      "Epoch   0 Batch   66/11516   train_loss = 7.341\n",
      "Epoch   0 Batch   67/11516   train_loss = 6.140\n",
      "Epoch   0 Batch   68/11516   train_loss = 4.779\n",
      "Epoch   0 Batch   69/11516   train_loss = 5.636\n",
      "Epoch   0 Batch   70/11516   train_loss = 5.317\n",
      "Epoch   0 Batch   71/11516   train_loss = 5.176\n",
      "Epoch   0 Batch   72/11516   train_loss = 8.780\n",
      "Epoch   0 Batch   73/11516   train_loss = 7.807\n",
      "Epoch   0 Batch   74/11516   train_loss = 5.803\n",
      "Epoch   0 Batch   75/11516   train_loss = 8.333\n",
      "Epoch   0 Batch   76/11516   train_loss = 4.843\n",
      "Epoch   0 Batch   77/11516   train_loss = 4.722\n",
      "Epoch   0 Batch   78/11516   train_loss = 5.618\n",
      "Epoch   0 Batch   79/11516   train_loss = 7.035\n",
      "Epoch   0 Batch   80/11516   train_loss = 5.646\n",
      "Epoch   0 Batch   81/11516   train_loss = 6.180\n",
      "Epoch   0 Batch   82/11516   train_loss = 6.874\n",
      "Epoch   0 Batch   83/11516   train_loss = 5.607\n",
      "Epoch   0 Batch   84/11516   train_loss = 6.496\n",
      "Epoch   0 Batch   85/11516   train_loss = 8.846\n",
      "Epoch   0 Batch   86/11516   train_loss = 5.525\n",
      "Epoch   0 Batch   87/11516   train_loss = 8.200\n",
      "Epoch   0 Batch   88/11516   train_loss = 6.475\n",
      "Epoch   0 Batch   89/11516   train_loss = 4.946\n",
      "Epoch   0 Batch   90/11516   train_loss = 9.465\n",
      "Epoch   0 Batch   91/11516   train_loss = 7.575\n",
      "Epoch   0 Batch   92/11516   train_loss = 7.108\n",
      "Epoch   0 Batch   93/11516   train_loss = 6.258\n",
      "Epoch   0 Batch   94/11516   train_loss = 5.500\n",
      "Epoch   0 Batch   95/11516   train_loss = 7.110\n",
      "Epoch   0 Batch   96/11516   train_loss = 6.296\n",
      "Epoch   0 Batch   97/11516   train_loss = 6.314\n",
      "Epoch   0 Batch   98/11516   train_loss = 6.261\n",
      "Epoch   0 Batch   99/11516   train_loss = 8.284\n",
      "Epoch   0 Batch  100/11516   train_loss = 5.978\n",
      "Epoch   0 Batch  101/11516   train_loss = 8.430\n",
      "Epoch   0 Batch  102/11516   train_loss = 6.154\n",
      "Epoch   0 Batch  103/11516   train_loss = 9.141\n",
      "Epoch   0 Batch  104/11516   train_loss = 6.999\n",
      "Epoch   0 Batch  105/11516   train_loss = 7.141\n",
      "Epoch   0 Batch  106/11516   train_loss = 6.693\n",
      "Epoch   0 Batch  107/11516   train_loss = 5.930\n",
      "Epoch   0 Batch  108/11516   train_loss = 6.671\n",
      "Epoch   0 Batch  109/11516   train_loss = 9.362\n",
      "Epoch   0 Batch  110/11516   train_loss = 7.135\n",
      "Epoch   0 Batch  111/11516   train_loss = 4.348\n",
      "Epoch   0 Batch  112/11516   train_loss = 5.310\n",
      "Epoch   0 Batch  113/11516   train_loss = 6.261\n",
      "Epoch   0 Batch  114/11516   train_loss = 5.019\n",
      "Epoch   0 Batch  115/11516   train_loss = 6.059\n",
      "Epoch   0 Batch  116/11516   train_loss = 6.147\n",
      "Epoch   0 Batch  117/11516   train_loss = 8.225\n",
      "Epoch   0 Batch  118/11516   train_loss = 7.985\n",
      "Epoch   0 Batch  119/11516   train_loss = 5.276\n",
      "Epoch   0 Batch  120/11516   train_loss = 4.689\n",
      "Epoch   0 Batch  121/11516   train_loss = 4.970\n",
      "Epoch   0 Batch  122/11516   train_loss = 6.801\n",
      "Epoch   0 Batch  123/11516   train_loss = 6.009\n",
      "Epoch   0 Batch  124/11516   train_loss = 5.724\n",
      "Epoch   0 Batch  125/11516   train_loss = 7.508\n",
      "Epoch   0 Batch  126/11516   train_loss = 4.389\n",
      "Epoch   0 Batch  127/11516   train_loss = 7.478\n",
      "Epoch   0 Batch  128/11516   train_loss = 7.168\n",
      "Epoch   0 Batch  129/11516   train_loss = 7.872\n",
      "Epoch   0 Batch  130/11516   train_loss = 5.230\n",
      "Epoch   0 Batch  131/11516   train_loss = 8.335\n",
      "Epoch   0 Batch  132/11516   train_loss = 5.374\n",
      "Epoch   0 Batch  133/11516   train_loss = 6.134\n",
      "Epoch   0 Batch  134/11516   train_loss = 3.935\n",
      "Epoch   0 Batch  135/11516   train_loss = 7.203\n",
      "Epoch   0 Batch  136/11516   train_loss = 6.372\n",
      "Epoch   0 Batch  137/11516   train_loss = 5.530\n",
      "Epoch   0 Batch  138/11516   train_loss = 6.717\n",
      "Epoch   0 Batch  139/11516   train_loss = 7.837\n",
      "Epoch   0 Batch  140/11516   train_loss = 7.934\n",
      "Epoch   0 Batch  141/11516   train_loss = 5.266\n",
      "Epoch   0 Batch  142/11516   train_loss = 7.893\n",
      "Epoch   0 Batch  143/11516   train_loss = 5.496\n",
      "Epoch   0 Batch  144/11516   train_loss = 8.294\n",
      "Epoch   0 Batch  145/11516   train_loss = 9.704\n",
      "Epoch   0 Batch  146/11516   train_loss = 9.358\n",
      "Epoch   0 Batch  147/11516   train_loss = 5.863\n",
      "Epoch   0 Batch  148/11516   train_loss = 9.336\n",
      "Epoch   0 Batch  149/11516   train_loss = 6.523\n",
      "Epoch   0 Batch  150/11516   train_loss = 8.416\n",
      "Epoch   0 Batch  151/11516   train_loss = 4.606\n",
      "Epoch   0 Batch  152/11516   train_loss = 9.217\n",
      "Epoch   0 Batch  153/11516   train_loss = 6.472\n",
      "Epoch   0 Batch  154/11516   train_loss = 8.303\n",
      "Epoch   0 Batch  155/11516   train_loss = 6.767\n",
      "Epoch   0 Batch  156/11516   train_loss = 7.745\n",
      "Epoch   0 Batch  157/11516   train_loss = 8.008\n",
      "Epoch   0 Batch  158/11516   train_loss = 6.789\n",
      "Epoch   0 Batch  159/11516   train_loss = 5.860\n",
      "Epoch   0 Batch  160/11516   train_loss = 5.439\n",
      "Epoch   0 Batch  161/11516   train_loss = 5.765\n",
      "Epoch   0 Batch  162/11516   train_loss = 7.537\n",
      "Epoch   0 Batch  163/11516   train_loss = 8.978\n",
      "Epoch   0 Batch  164/11516   train_loss = 6.605\n",
      "Epoch   0 Batch  165/11516   train_loss = 9.112\n",
      "Epoch   0 Batch  166/11516   train_loss = 6.647\n",
      "Epoch   0 Batch  167/11516   train_loss = 5.824\n",
      "Epoch   0 Batch  168/11516   train_loss = 5.935\n",
      "Epoch   0 Batch  169/11516   train_loss = 6.721\n",
      "Epoch   0 Batch  170/11516   train_loss = 6.796\n",
      "Epoch   0 Batch  171/11516   train_loss = 4.859\n",
      "Epoch   0 Batch  172/11516   train_loss = 5.999\n",
      "Epoch   0 Batch  173/11516   train_loss = 4.153\n",
      "Epoch   0 Batch  174/11516   train_loss = 4.796\n",
      "Epoch   0 Batch  175/11516   train_loss = 6.486\n",
      "Epoch   0 Batch  176/11516   train_loss = 8.519\n",
      "Epoch   0 Batch  177/11516   train_loss = 6.733\n",
      "Epoch   0 Batch  178/11516   train_loss = 6.207\n",
      "Epoch   0 Batch  179/11516   train_loss = 5.557\n",
      "Epoch   0 Batch  180/11516   train_loss = 5.511\n",
      "Epoch   0 Batch  181/11516   train_loss = 7.660\n",
      "Epoch   0 Batch  182/11516   train_loss = 6.323\n",
      "Epoch   0 Batch  183/11516   train_loss = 6.320\n",
      "Epoch   0 Batch  184/11516   train_loss = 7.213\n",
      "Epoch   0 Batch  185/11516   train_loss = 5.228\n",
      "Epoch   0 Batch  186/11516   train_loss = 6.982\n",
      "Epoch   0 Batch  187/11516   train_loss = 4.638\n",
      "Epoch   0 Batch  188/11516   train_loss = 5.654\n",
      "Epoch   0 Batch  189/11516   train_loss = 6.188\n",
      "Epoch   0 Batch  190/11516   train_loss = 7.106\n",
      "Epoch   0 Batch  191/11516   train_loss = 5.979\n",
      "Epoch   0 Batch  192/11516   train_loss = 5.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  193/11516   train_loss = 5.831\n",
      "Epoch   0 Batch  194/11516   train_loss = 8.244\n",
      "Epoch   0 Batch  195/11516   train_loss = 6.326\n",
      "Epoch   0 Batch  196/11516   train_loss = 5.074\n",
      "Epoch   0 Batch  197/11516   train_loss = 5.409\n",
      "Epoch   0 Batch  198/11516   train_loss = 6.831\n",
      "Epoch   0 Batch  199/11516   train_loss = 6.211\n",
      "Epoch   0 Batch  200/11516   train_loss = 8.020\n",
      "Epoch   0 Batch  201/11516   train_loss = 9.134\n",
      "Epoch   0 Batch  202/11516   train_loss = 4.677\n",
      "Epoch   0 Batch  203/11516   train_loss = 6.909\n",
      "Epoch   0 Batch  204/11516   train_loss = 5.952\n",
      "Epoch   0 Batch  205/11516   train_loss = 4.828\n",
      "Epoch   0 Batch  206/11516   train_loss = 6.724\n",
      "Epoch   0 Batch  207/11516   train_loss = 6.960\n",
      "Epoch   0 Batch  208/11516   train_loss = 6.073\n",
      "Epoch   0 Batch  209/11516   train_loss = 7.712\n",
      "Epoch   0 Batch  210/11516   train_loss = 7.538\n",
      "Epoch   0 Batch  211/11516   train_loss = 7.103\n",
      "Epoch   0 Batch  212/11516   train_loss = 4.755\n",
      "Epoch   0 Batch  213/11516   train_loss = 7.976\n",
      "Epoch   0 Batch  214/11516   train_loss = 5.969\n",
      "Epoch   0 Batch  215/11516   train_loss = 6.147\n",
      "Epoch   0 Batch  216/11516   train_loss = 6.024\n",
      "Epoch   0 Batch  217/11516   train_loss = 7.548\n",
      "Epoch   0 Batch  218/11516   train_loss = 7.113\n",
      "Epoch   0 Batch  219/11516   train_loss = 5.462\n",
      "Epoch   0 Batch  220/11516   train_loss = 6.785\n",
      "Epoch   0 Batch  221/11516   train_loss = 5.366\n",
      "Epoch   0 Batch  222/11516   train_loss = 8.208\n",
      "Epoch   0 Batch  223/11516   train_loss = 8.017\n",
      "Epoch   0 Batch  224/11516   train_loss = 4.730\n",
      "Epoch   0 Batch  225/11516   train_loss = 5.380\n",
      "Epoch   0 Batch  226/11516   train_loss = 8.107\n",
      "Epoch   0 Batch  227/11516   train_loss = 5.653\n",
      "Epoch   0 Batch  228/11516   train_loss = 8.981\n",
      "Epoch   0 Batch  229/11516   train_loss = 7.104\n",
      "Epoch   0 Batch  230/11516   train_loss = 6.908\n",
      "Epoch   0 Batch  231/11516   train_loss = 9.409\n",
      "Epoch   0 Batch  232/11516   train_loss = 8.174\n",
      "Epoch   0 Batch  233/11516   train_loss = 8.104\n",
      "Epoch   0 Batch  234/11516   train_loss = 8.256\n",
      "Epoch   0 Batch  235/11516   train_loss = 5.878\n",
      "Epoch   0 Batch  236/11516   train_loss = 5.932\n",
      "Epoch   0 Batch  237/11516   train_loss = 6.743\n",
      "Epoch   0 Batch  238/11516   train_loss = 5.930\n",
      "Epoch   0 Batch  239/11516   train_loss = 6.618\n",
      "Epoch   0 Batch  240/11516   train_loss = 6.709\n",
      "Epoch   0 Batch  241/11516   train_loss = 5.300\n",
      "Epoch   0 Batch  242/11516   train_loss = 6.260\n",
      "Epoch   0 Batch  243/11516   train_loss = 6.463\n",
      "Epoch   0 Batch  244/11516   train_loss = 7.522\n",
      "Epoch   0 Batch  245/11516   train_loss = 6.298\n",
      "Epoch   0 Batch  246/11516   train_loss = 6.528\n",
      "Epoch   0 Batch  247/11516   train_loss = 4.081\n",
      "Epoch   0 Batch  248/11516   train_loss = 4.456\n",
      "Epoch   0 Batch  249/11516   train_loss = 6.252\n",
      "Epoch   0 Batch  250/11516   train_loss = 5.675\n",
      "Epoch   0 Batch  251/11516   train_loss = 7.040\n",
      "Epoch   0 Batch  252/11516   train_loss = 7.614\n",
      "Epoch   0 Batch  253/11516   train_loss = 4.685\n",
      "Epoch   0 Batch  254/11516   train_loss = 6.829\n",
      "Epoch   0 Batch  255/11516   train_loss = 6.809\n",
      "Epoch   0 Batch  256/11516   train_loss = 5.632\n",
      "Epoch   0 Batch  257/11516   train_loss = 8.026\n",
      "Epoch   0 Batch  258/11516   train_loss = 7.212\n",
      "Epoch   0 Batch  259/11516   train_loss = 5.251\n",
      "Epoch   0 Batch  260/11516   train_loss = 7.640\n",
      "Epoch   0 Batch  261/11516   train_loss = 6.999\n",
      "Epoch   0 Batch  262/11516   train_loss = 7.336\n",
      "Epoch   0 Batch  263/11516   train_loss = 5.449\n",
      "Epoch   0 Batch  264/11516   train_loss = 6.336\n",
      "Epoch   0 Batch  265/11516   train_loss = 6.620\n",
      "Epoch   0 Batch  266/11516   train_loss = 3.897\n",
      "Epoch   0 Batch  267/11516   train_loss = 7.221\n",
      "Epoch   0 Batch  268/11516   train_loss = 7.688\n",
      "Epoch   0 Batch  269/11516   train_loss = 4.184\n",
      "Epoch   0 Batch  270/11516   train_loss = 7.164\n",
      "Epoch   0 Batch  271/11516   train_loss = 7.457\n",
      "Epoch   0 Batch  272/11516   train_loss = 6.471\n",
      "Epoch   0 Batch  273/11516   train_loss = 7.379\n",
      "Epoch   0 Batch  274/11516   train_loss = 6.108\n",
      "Epoch   0 Batch  275/11516   train_loss = 9.709\n",
      "Epoch   0 Batch  276/11516   train_loss = 5.054\n",
      "Epoch   0 Batch  277/11516   train_loss = 4.785\n",
      "Epoch   0 Batch  278/11516   train_loss = 7.953\n",
      "Epoch   0 Batch  279/11516   train_loss = 7.104\n",
      "Epoch   0 Batch  280/11516   train_loss = 4.644\n",
      "Epoch   0 Batch  281/11516   train_loss = 6.868\n",
      "Epoch   0 Batch  282/11516   train_loss = 7.130\n",
      "Epoch   0 Batch  283/11516   train_loss = 6.928\n",
      "Epoch   0 Batch  284/11516   train_loss = 4.751\n",
      "Epoch   0 Batch  285/11516   train_loss = 7.377\n",
      "Epoch   0 Batch  286/11516   train_loss = 6.770\n",
      "Epoch   0 Batch  287/11516   train_loss = 4.177\n",
      "Epoch   0 Batch  288/11516   train_loss = 5.172\n",
      "Epoch   0 Batch  289/11516   train_loss = 5.558\n",
      "Epoch   0 Batch  290/11516   train_loss = 7.607\n",
      "Epoch   0 Batch  291/11516   train_loss = 6.787\n",
      "Epoch   0 Batch  292/11516   train_loss = 4.856\n",
      "Epoch   0 Batch  293/11516   train_loss = 5.377\n",
      "Epoch   0 Batch  294/11516   train_loss = 7.756\n",
      "Epoch   0 Batch  295/11516   train_loss = 7.156\n",
      "Epoch   0 Batch  296/11516   train_loss = 7.152\n",
      "Epoch   0 Batch  297/11516   train_loss = 5.321\n",
      "Epoch   0 Batch  298/11516   train_loss = 3.759\n",
      "Epoch   0 Batch  299/11516   train_loss = 5.843\n",
      "Epoch   0 Batch  300/11516   train_loss = 4.918\n",
      "Epoch   0 Batch  301/11516   train_loss = 6.596\n",
      "Epoch   0 Batch  302/11516   train_loss = 8.688\n",
      "Epoch   0 Batch  303/11516   train_loss = 9.102\n",
      "Epoch   0 Batch  304/11516   train_loss = 5.219\n",
      "Epoch   0 Batch  305/11516   train_loss = 6.679\n",
      "Epoch   0 Batch  306/11516   train_loss = 3.766\n",
      "Epoch   0 Batch  307/11516   train_loss = 8.274\n",
      "Epoch   0 Batch  308/11516   train_loss = 6.518\n",
      "Epoch   0 Batch  309/11516   train_loss = 7.488\n",
      "Epoch   0 Batch  310/11516   train_loss = 5.109\n",
      "Epoch   0 Batch  311/11516   train_loss = 4.127\n",
      "Epoch   0 Batch  312/11516   train_loss = 4.345\n",
      "Epoch   0 Batch  313/11516   train_loss = 6.926\n",
      "Epoch   0 Batch  314/11516   train_loss = 9.225\n",
      "Epoch   0 Batch  315/11516   train_loss = 3.192\n",
      "Epoch   0 Batch  316/11516   train_loss = 6.889\n",
      "Epoch   0 Batch  317/11516   train_loss = 6.677\n",
      "Epoch   0 Batch  318/11516   train_loss = 7.730\n",
      "Epoch   0 Batch  319/11516   train_loss = 7.569\n",
      "Epoch   0 Batch  320/11516   train_loss = 5.100\n",
      "Epoch   0 Batch  321/11516   train_loss = 4.265\n",
      "Epoch   0 Batch  322/11516   train_loss = 5.315\n",
      "Epoch   0 Batch  323/11516   train_loss = 7.708\n",
      "Epoch   0 Batch  324/11516   train_loss = 6.490\n",
      "Epoch   0 Batch  325/11516   train_loss = 6.908\n",
      "Epoch   0 Batch  326/11516   train_loss = 6.698\n",
      "Epoch   0 Batch  327/11516   train_loss = 4.630\n",
      "Epoch   0 Batch  328/11516   train_loss = 6.439\n",
      "Epoch   0 Batch  329/11516   train_loss = 5.799\n",
      "Epoch   0 Batch  330/11516   train_loss = 4.724\n",
      "Epoch   0 Batch  331/11516   train_loss = 7.641\n",
      "Epoch   0 Batch  332/11516   train_loss = 7.551\n",
      "Epoch   0 Batch  333/11516   train_loss = 7.936\n",
      "Epoch   0 Batch  334/11516   train_loss = 7.593\n",
      "Epoch   0 Batch  335/11516   train_loss = 6.127\n",
      "Epoch   0 Batch  336/11516   train_loss = 6.974\n",
      "Epoch   0 Batch  337/11516   train_loss = 3.881\n",
      "Epoch   0 Batch  338/11516   train_loss = 6.429\n",
      "Epoch   0 Batch  339/11516   train_loss = 8.142\n",
      "Epoch   0 Batch  340/11516   train_loss = 5.002\n",
      "Epoch   0 Batch  341/11516   train_loss = 4.905\n",
      "Epoch   0 Batch  342/11516   train_loss = 8.646\n",
      "Epoch   0 Batch  343/11516   train_loss = 6.076\n",
      "Epoch   0 Batch  344/11516   train_loss = 3.805\n",
      "Epoch   0 Batch  345/11516   train_loss = 4.779\n",
      "Epoch   0 Batch  346/11516   train_loss = 5.792\n",
      "Epoch   0 Batch  347/11516   train_loss = 7.122\n",
      "Epoch   0 Batch  348/11516   train_loss = 6.990\n",
      "Epoch   0 Batch  349/11516   train_loss = 4.221\n",
      "Epoch   0 Batch  350/11516   train_loss = 5.194\n",
      "Epoch   0 Batch  351/11516   train_loss = 5.827\n",
      "Epoch   0 Batch  352/11516   train_loss = 5.001\n",
      "Epoch   0 Batch  353/11516   train_loss = 7.056\n",
      "Epoch   0 Batch  354/11516   train_loss = 7.459\n",
      "Epoch   0 Batch  355/11516   train_loss = 3.286\n",
      "Epoch   0 Batch  356/11516   train_loss = 7.470\n",
      "Epoch   0 Batch  357/11516   train_loss = 7.146\n",
      "Epoch   0 Batch  358/11516   train_loss = 7.479\n",
      "Epoch   0 Batch  359/11516   train_loss = 4.697\n",
      "Epoch   0 Batch  360/11516   train_loss = 5.815\n",
      "Epoch   0 Batch  361/11516   train_loss = 4.457\n",
      "Epoch   0 Batch  362/11516   train_loss = 4.037\n",
      "Epoch   0 Batch  363/11516   train_loss = 4.735\n",
      "Epoch   0 Batch  364/11516   train_loss = 8.866\n",
      "Epoch   0 Batch  365/11516   train_loss = 8.136\n",
      "Epoch   0 Batch  366/11516   train_loss = 7.036\n",
      "Epoch   0 Batch  367/11516   train_loss = 4.694\n",
      "Epoch   0 Batch  368/11516   train_loss = 5.871\n",
      "Epoch   0 Batch  369/11516   train_loss = 8.940\n",
      "Epoch   0 Batch  370/11516   train_loss = 6.726\n",
      "Epoch   0 Batch  371/11516   train_loss = 7.027\n",
      "Epoch   0 Batch  372/11516   train_loss = 7.621\n",
      "Epoch   0 Batch  373/11516   train_loss = 5.359\n",
      "Epoch   0 Batch  374/11516   train_loss = 4.207\n",
      "Epoch   0 Batch  375/11516   train_loss = 6.074\n",
      "Epoch   0 Batch  376/11516   train_loss = 6.857\n",
      "Epoch   0 Batch  377/11516   train_loss = 6.305\n",
      "Epoch   0 Batch  378/11516   train_loss = 8.239\n",
      "Epoch   0 Batch  379/11516   train_loss = 11.250\n",
      "Epoch   0 Batch  380/11516   train_loss = 6.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  381/11516   train_loss = 6.134\n",
      "Epoch   0 Batch  382/11516   train_loss = 8.970\n",
      "Epoch   0 Batch  383/11516   train_loss = 5.627\n",
      "Epoch   0 Batch  384/11516   train_loss = 6.692\n",
      "Epoch   0 Batch  385/11516   train_loss = 5.498\n",
      "Epoch   0 Batch  386/11516   train_loss = 6.605\n",
      "Epoch   0 Batch  387/11516   train_loss = 7.156\n",
      "Epoch   0 Batch  388/11516   train_loss = 6.157\n",
      "Epoch   0 Batch  389/11516   train_loss = 5.618\n",
      "Epoch   0 Batch  390/11516   train_loss = 8.498\n",
      "Epoch   0 Batch  391/11516   train_loss = 8.806\n",
      "Epoch   0 Batch  392/11516   train_loss = 7.618\n",
      "Epoch   0 Batch  393/11516   train_loss = 5.113\n",
      "Epoch   0 Batch  394/11516   train_loss = 5.163\n",
      "Epoch   0 Batch  395/11516   train_loss = 8.953\n",
      "Epoch   0 Batch  396/11516   train_loss = 5.367\n",
      "Epoch   0 Batch  397/11516   train_loss = 7.545\n",
      "Epoch   0 Batch  398/11516   train_loss = 7.270\n",
      "Epoch   0 Batch  399/11516   train_loss = 5.377\n",
      "Epoch   0 Batch  400/11516   train_loss = 6.568\n",
      "Epoch   0 Batch  401/11516   train_loss = 4.518\n",
      "Epoch   0 Batch  402/11516   train_loss = 6.857\n",
      "Epoch   0 Batch  403/11516   train_loss = 4.903\n",
      "Epoch   0 Batch  404/11516   train_loss = 6.087\n",
      "Epoch   0 Batch  405/11516   train_loss = 5.385\n",
      "Epoch   0 Batch  406/11516   train_loss = 8.062\n",
      "Epoch   0 Batch  407/11516   train_loss = 6.674\n",
      "Epoch   0 Batch  408/11516   train_loss = 5.042\n",
      "Epoch   0 Batch  409/11516   train_loss = 7.819\n",
      "Epoch   0 Batch  410/11516   train_loss = 4.079\n",
      "Epoch   0 Batch  411/11516   train_loss = 7.783\n",
      "Epoch   0 Batch  412/11516   train_loss = 5.845\n",
      "Epoch   0 Batch  413/11516   train_loss = 5.568\n",
      "Epoch   0 Batch  414/11516   train_loss = 6.287\n",
      "Epoch   0 Batch  415/11516   train_loss = 6.446\n",
      "Epoch   0 Batch  416/11516   train_loss = 6.173\n",
      "Epoch   0 Batch  417/11516   train_loss = 8.741\n",
      "Epoch   0 Batch  418/11516   train_loss = 6.638\n",
      "Epoch   0 Batch  419/11516   train_loss = 6.675\n",
      "Epoch   0 Batch  420/11516   train_loss = 6.098\n",
      "Epoch   0 Batch  421/11516   train_loss = 5.518\n",
      "Epoch   0 Batch  422/11516   train_loss = 5.400\n",
      "Epoch   0 Batch  423/11516   train_loss = 7.236\n",
      "Epoch   0 Batch  424/11516   train_loss = 6.519\n",
      "Epoch   0 Batch  425/11516   train_loss = 6.025\n",
      "Epoch   0 Batch  426/11516   train_loss = 6.998\n",
      "Epoch   0 Batch  427/11516   train_loss = 6.571\n",
      "Epoch   0 Batch  428/11516   train_loss = 8.926\n",
      "Epoch   0 Batch  429/11516   train_loss = 6.375\n",
      "Epoch   0 Batch  430/11516   train_loss = 8.804\n",
      "Epoch   0 Batch  431/11516   train_loss = 4.933\n",
      "Epoch   0 Batch  432/11516   train_loss = 4.669\n",
      "Epoch   0 Batch  433/11516   train_loss = 7.224\n",
      "Epoch   0 Batch  434/11516   train_loss = 5.354\n",
      "Epoch   0 Batch  435/11516   train_loss = 6.274\n",
      "Epoch   0 Batch  436/11516   train_loss = 7.577\n",
      "Epoch   0 Batch  437/11516   train_loss = 7.161\n",
      "Epoch   0 Batch  438/11516   train_loss = 3.867\n",
      "Epoch   0 Batch  439/11516   train_loss = 7.416\n",
      "Epoch   0 Batch  440/11516   train_loss = 8.820\n",
      "Epoch   0 Batch  441/11516   train_loss = 4.535\n",
      "Epoch   0 Batch  442/11516   train_loss = 5.908\n",
      "Epoch   0 Batch  443/11516   train_loss = 5.124\n",
      "Epoch   0 Batch  444/11516   train_loss = 6.891\n",
      "Epoch   0 Batch  445/11516   train_loss = 5.569\n",
      "Epoch   0 Batch  446/11516   train_loss = 4.807\n",
      "Epoch   0 Batch  447/11516   train_loss = 8.226\n",
      "Epoch   0 Batch  448/11516   train_loss = 7.576\n",
      "Epoch   0 Batch  449/11516   train_loss = 6.621\n",
      "Epoch   0 Batch  450/11516   train_loss = 7.051\n",
      "Epoch   0 Batch  451/11516   train_loss = 5.275\n",
      "Epoch   0 Batch  452/11516   train_loss = 7.416\n",
      "Epoch   0 Batch  453/11516   train_loss = 4.282\n",
      "Epoch   0 Batch  454/11516   train_loss = 7.580\n",
      "Epoch   0 Batch  455/11516   train_loss = 5.340\n",
      "Epoch   0 Batch  456/11516   train_loss = 5.737\n",
      "Epoch   0 Batch  457/11516   train_loss = 6.838\n",
      "Epoch   0 Batch  458/11516   train_loss = 6.889\n",
      "Epoch   0 Batch  459/11516   train_loss = 6.557\n",
      "Epoch   0 Batch  460/11516   train_loss = 8.372\n",
      "Epoch   0 Batch  461/11516   train_loss = 6.163\n",
      "Epoch   0 Batch  462/11516   train_loss = 8.041\n",
      "Epoch   0 Batch  463/11516   train_loss = 7.071\n",
      "Epoch   0 Batch  464/11516   train_loss = 4.221\n",
      "Epoch   0 Batch  465/11516   train_loss = 5.791\n",
      "Epoch   0 Batch  466/11516   train_loss = 7.302\n",
      "Epoch   0 Batch  467/11516   train_loss = 7.029\n",
      "Epoch   0 Batch  468/11516   train_loss = 5.857\n",
      "Epoch   0 Batch  469/11516   train_loss = 6.961\n",
      "Epoch   0 Batch  470/11516   train_loss = 5.844\n",
      "Epoch   0 Batch  471/11516   train_loss = 7.241\n",
      "Epoch   0 Batch  472/11516   train_loss = 5.307\n",
      "Epoch   0 Batch  473/11516   train_loss = 9.260\n",
      "Epoch   0 Batch  474/11516   train_loss = 6.681\n",
      "Epoch   0 Batch  475/11516   train_loss = 5.041\n",
      "Epoch   0 Batch  476/11516   train_loss = 3.836\n",
      "Epoch   0 Batch  477/11516   train_loss = 6.820\n",
      "Epoch   0 Batch  478/11516   train_loss = 5.564\n",
      "Epoch   0 Batch  479/11516   train_loss = 4.848\n",
      "Epoch   0 Batch  480/11516   train_loss = 4.742\n",
      "Epoch   0 Batch  481/11516   train_loss = 7.186\n",
      "Epoch   0 Batch  482/11516   train_loss = 6.865\n",
      "Epoch   0 Batch  483/11516   train_loss = 6.169\n",
      "Epoch   0 Batch  484/11516   train_loss = 6.536\n",
      "Epoch   0 Batch  485/11516   train_loss = 3.614\n",
      "Epoch   0 Batch  486/11516   train_loss = 8.375\n",
      "Epoch   0 Batch  487/11516   train_loss = 6.562\n",
      "Epoch   0 Batch  488/11516   train_loss = 4.339\n",
      "Epoch   0 Batch  489/11516   train_loss = 4.477\n",
      "Epoch   0 Batch  490/11516   train_loss = 7.703\n",
      "Epoch   0 Batch  491/11516   train_loss = 7.823\n",
      "Epoch   0 Batch  492/11516   train_loss = 6.145\n",
      "Epoch   0 Batch  493/11516   train_loss = 5.976\n",
      "Epoch   0 Batch  494/11516   train_loss = 6.870\n",
      "Epoch   0 Batch  495/11516   train_loss = 6.735\n",
      "Epoch   0 Batch  496/11516   train_loss = 8.152\n",
      "Epoch   0 Batch  497/11516   train_loss = 9.563\n",
      "Epoch   0 Batch  498/11516   train_loss = 5.503\n",
      "Epoch   0 Batch  499/11516   train_loss = 6.470\n",
      "Epoch   0 Batch  500/11516   train_loss = 7.184\n",
      "Epoch   0 Batch  501/11516   train_loss = 8.375\n",
      "Epoch   0 Batch  502/11516   train_loss = 7.129\n",
      "Epoch   0 Batch  503/11516   train_loss = 3.154\n",
      "Epoch   0 Batch  504/11516   train_loss = 6.772\n",
      "Epoch   0 Batch  505/11516   train_loss = 6.436\n",
      "Epoch   0 Batch  506/11516   train_loss = 7.425\n",
      "Epoch   0 Batch  507/11516   train_loss = 5.593\n",
      "Epoch   0 Batch  508/11516   train_loss = 7.957\n",
      "Epoch   0 Batch  509/11516   train_loss = 7.741\n",
      "Epoch   0 Batch  510/11516   train_loss = 6.272\n",
      "Epoch   0 Batch  511/11516   train_loss = 7.704\n",
      "Epoch   0 Batch  512/11516   train_loss = 8.350\n",
      "Epoch   0 Batch  513/11516   train_loss = 4.334\n",
      "Epoch   0 Batch  514/11516   train_loss = 5.452\n",
      "Epoch   0 Batch  515/11516   train_loss = 6.945\n",
      "Epoch   0 Batch  516/11516   train_loss = 3.352\n",
      "Epoch   0 Batch  517/11516   train_loss = 7.024\n",
      "Epoch   0 Batch  518/11516   train_loss = 6.935\n",
      "Epoch   0 Batch  519/11516   train_loss = 5.004\n",
      "Epoch   0 Batch  520/11516   train_loss = 3.505\n",
      "Epoch   0 Batch  521/11516   train_loss = 7.393\n",
      "Epoch   0 Batch  522/11516   train_loss = 6.153\n",
      "Epoch   0 Batch  523/11516   train_loss = 9.917\n",
      "Epoch   0 Batch  524/11516   train_loss = 6.721\n",
      "Epoch   0 Batch  525/11516   train_loss = 4.584\n",
      "Epoch   0 Batch  526/11516   train_loss = 8.542\n",
      "Epoch   0 Batch  527/11516   train_loss = 8.798\n",
      "Epoch   0 Batch  528/11516   train_loss = 8.460\n",
      "Epoch   0 Batch  529/11516   train_loss = 5.455\n",
      "Epoch   0 Batch  530/11516   train_loss = 8.117\n",
      "Epoch   0 Batch  531/11516   train_loss = 6.618\n",
      "Epoch   0 Batch  532/11516   train_loss = 5.148\n",
      "Epoch   0 Batch  533/11516   train_loss = 4.138\n",
      "Epoch   0 Batch  534/11516   train_loss = 5.065\n",
      "Epoch   0 Batch  535/11516   train_loss = 5.397\n",
      "Epoch   0 Batch  536/11516   train_loss = 4.817\n",
      "Epoch   0 Batch  537/11516   train_loss = 5.596\n",
      "Epoch   0 Batch  538/11516   train_loss = 7.521\n",
      "Epoch   0 Batch  539/11516   train_loss = 4.743\n",
      "Epoch   0 Batch  540/11516   train_loss = 5.760\n",
      "Epoch   0 Batch  541/11516   train_loss = 6.284\n",
      "Epoch   0 Batch  542/11516   train_loss = 7.257\n",
      "Epoch   0 Batch  543/11516   train_loss = 5.440\n",
      "Epoch   0 Batch  544/11516   train_loss = 3.550\n",
      "Epoch   0 Batch  545/11516   train_loss = 3.767\n",
      "Epoch   0 Batch  546/11516   train_loss = 4.775\n",
      "Epoch   0 Batch  547/11516   train_loss = 4.604\n",
      "Epoch   0 Batch  548/11516   train_loss = 5.911\n",
      "Epoch   0 Batch  549/11516   train_loss = 5.962\n",
      "Epoch   0 Batch  550/11516   train_loss = 8.242\n",
      "Epoch   0 Batch  551/11516   train_loss = 5.886\n",
      "Epoch   0 Batch  552/11516   train_loss = 8.840\n",
      "Epoch   0 Batch  553/11516   train_loss = 4.733\n",
      "Epoch   0 Batch  554/11516   train_loss = 5.695\n",
      "Epoch   0 Batch  555/11516   train_loss = 6.289\n",
      "Epoch   0 Batch  556/11516   train_loss = 10.215\n",
      "Epoch   0 Batch  557/11516   train_loss = 6.185\n",
      "Epoch   0 Batch  558/11516   train_loss = 6.685\n",
      "Epoch   0 Batch  559/11516   train_loss = 7.913\n",
      "Epoch   0 Batch  560/11516   train_loss = 4.165\n",
      "Epoch   0 Batch  561/11516   train_loss = 6.287\n",
      "Epoch   0 Batch  562/11516   train_loss = 7.815\n",
      "Epoch   0 Batch  563/11516   train_loss = 5.720\n",
      "Epoch   0 Batch  564/11516   train_loss = 5.353\n",
      "Epoch   0 Batch  565/11516   train_loss = 6.855\n",
      "Epoch   0 Batch  566/11516   train_loss = 7.442\n",
      "Epoch   0 Batch  567/11516   train_loss = 6.000\n",
      "Epoch   0 Batch  568/11516   train_loss = 7.730\n",
      "Epoch   0 Batch  569/11516   train_loss = 6.307\n",
      "Epoch   0 Batch  570/11516   train_loss = 4.402\n",
      "Epoch   0 Batch  571/11516   train_loss = 4.657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  572/11516   train_loss = 7.142\n",
      "Epoch   0 Batch  573/11516   train_loss = 6.076\n",
      "Epoch   0 Batch  574/11516   train_loss = 7.104\n",
      "Epoch   0 Batch  575/11516   train_loss = 7.213\n",
      "Epoch   0 Batch  576/11516   train_loss = 6.510\n",
      "Epoch   0 Batch  577/11516   train_loss = 7.257\n",
      "Epoch   0 Batch  578/11516   train_loss = 7.281\n",
      "Epoch   0 Batch  579/11516   train_loss = 6.242\n",
      "Epoch   0 Batch  580/11516   train_loss = 3.695\n",
      "Epoch   0 Batch  581/11516   train_loss = 6.168\n",
      "Epoch   0 Batch  582/11516   train_loss = 8.793\n",
      "Epoch   0 Batch  583/11516   train_loss = 6.660\n",
      "Epoch   0 Batch  584/11516   train_loss = 3.313\n",
      "Epoch   0 Batch  585/11516   train_loss = 6.348\n",
      "Epoch   0 Batch  586/11516   train_loss = 5.691\n",
      "Epoch   0 Batch  587/11516   train_loss = 6.569\n",
      "Epoch   0 Batch  588/11516   train_loss = 9.988\n",
      "Epoch   0 Batch  589/11516   train_loss = 6.012\n",
      "Epoch   0 Batch  590/11516   train_loss = 5.717\n",
      "Epoch   0 Batch  591/11516   train_loss = 4.761\n",
      "Epoch   0 Batch  592/11516   train_loss = 6.269\n",
      "Epoch   0 Batch  593/11516   train_loss = 5.178\n",
      "Epoch   0 Batch  594/11516   train_loss = 6.984\n",
      "Epoch   0 Batch  595/11516   train_loss = 4.655\n",
      "Epoch   0 Batch  596/11516   train_loss = 8.658\n",
      "Epoch   0 Batch  597/11516   train_loss = 8.995\n",
      "Epoch   0 Batch  598/11516   train_loss = 5.653\n",
      "Epoch   0 Batch  599/11516   train_loss = 6.577\n",
      "Epoch   0 Batch  600/11516   train_loss = 4.843\n",
      "Epoch   0 Batch  601/11516   train_loss = 4.939\n",
      "Epoch   0 Batch  602/11516   train_loss = 6.683\n",
      "Epoch   0 Batch  603/11516   train_loss = 5.688\n",
      "Epoch   0 Batch  604/11516   train_loss = 5.791\n",
      "Epoch   0 Batch  605/11516   train_loss = 5.107\n",
      "Epoch   0 Batch  606/11516   train_loss = 4.822\n",
      "Epoch   0 Batch  607/11516   train_loss = 6.754\n",
      "Epoch   0 Batch  608/11516   train_loss = 3.548\n",
      "Epoch   0 Batch  609/11516   train_loss = 6.702\n",
      "Epoch   0 Batch  610/11516   train_loss = 4.981\n",
      "Epoch   0 Batch  611/11516   train_loss = 7.117\n",
      "Epoch   0 Batch  612/11516   train_loss = 5.696\n",
      "Epoch   0 Batch  613/11516   train_loss = 4.115\n",
      "Epoch   0 Batch  614/11516   train_loss = 8.115\n",
      "Epoch   0 Batch  615/11516   train_loss = 6.445\n",
      "Epoch   0 Batch  616/11516   train_loss = 3.278\n",
      "Epoch   0 Batch  617/11516   train_loss = 4.724\n",
      "Epoch   0 Batch  618/11516   train_loss = 5.733\n",
      "Epoch   0 Batch  619/11516   train_loss = 5.780\n",
      "Epoch   0 Batch  620/11516   train_loss = 9.726\n",
      "Epoch   0 Batch  621/11516   train_loss = 3.999\n",
      "Epoch   0 Batch  622/11516   train_loss = 4.843\n",
      "Epoch   0 Batch  623/11516   train_loss = 9.022\n",
      "Epoch   0 Batch  624/11516   train_loss = 8.436\n",
      "Epoch   0 Batch  625/11516   train_loss = 6.262\n",
      "Epoch   0 Batch  626/11516   train_loss = 2.708\n",
      "Epoch   0 Batch  627/11516   train_loss = 7.111\n",
      "Epoch   0 Batch  628/11516   train_loss = 6.359\n",
      "Epoch   0 Batch  629/11516   train_loss = 7.381\n",
      "Epoch   0 Batch  630/11516   train_loss = 6.931\n",
      "Epoch   0 Batch  631/11516   train_loss = 4.023\n",
      "Epoch   0 Batch  632/11516   train_loss = 6.564\n",
      "Epoch   0 Batch  633/11516   train_loss = 7.922\n",
      "Epoch   0 Batch  634/11516   train_loss = 6.832\n",
      "Epoch   0 Batch  635/11516   train_loss = 6.733\n",
      "Epoch   0 Batch  636/11516   train_loss = 8.402\n",
      "Epoch   0 Batch  637/11516   train_loss = 6.982\n",
      "Epoch   0 Batch  638/11516   train_loss = 7.138\n",
      "Epoch   0 Batch  639/11516   train_loss = 8.112\n",
      "Epoch   0 Batch  640/11516   train_loss = 3.726\n",
      "Epoch   0 Batch  641/11516   train_loss = 7.112\n",
      "Epoch   0 Batch  642/11516   train_loss = 8.280\n",
      "Epoch   0 Batch  643/11516   train_loss = 6.211\n",
      "Epoch   0 Batch  644/11516   train_loss = 8.872\n",
      "Epoch   0 Batch  645/11516   train_loss = 4.241\n",
      "Epoch   0 Batch  646/11516   train_loss = 7.474\n",
      "Epoch   0 Batch  647/11516   train_loss = 6.039\n",
      "Epoch   0 Batch  648/11516   train_loss = 5.943\n",
      "Epoch   0 Batch  649/11516   train_loss = 6.122\n",
      "Epoch   0 Batch  650/11516   train_loss = 4.233\n",
      "Epoch   0 Batch  651/11516   train_loss = 4.736\n",
      "Epoch   0 Batch  652/11516   train_loss = 7.506\n",
      "Epoch   0 Batch  653/11516   train_loss = 9.871\n",
      "Epoch   0 Batch  654/11516   train_loss = 3.768\n",
      "Epoch   0 Batch  655/11516   train_loss = 4.499\n",
      "Epoch   0 Batch  656/11516   train_loss = 8.369\n",
      "Epoch   0 Batch  657/11516   train_loss = 8.119\n",
      "Epoch   0 Batch  658/11516   train_loss = 3.637\n",
      "Epoch   0 Batch  659/11516   train_loss = 5.388\n",
      "Epoch   0 Batch  660/11516   train_loss = 5.333\n",
      "Epoch   0 Batch  661/11516   train_loss = 7.212\n",
      "Epoch   0 Batch  662/11516   train_loss = 5.557\n",
      "Epoch   0 Batch  663/11516   train_loss = 6.251\n",
      "Epoch   0 Batch  664/11516   train_loss = 5.359\n",
      "Epoch   0 Batch  665/11516   train_loss = 7.311\n",
      "Epoch   0 Batch  666/11516   train_loss = 6.670\n",
      "Epoch   0 Batch  667/11516   train_loss = 8.301\n",
      "Epoch   0 Batch  668/11516   train_loss = 5.190\n",
      "Epoch   0 Batch  669/11516   train_loss = 5.351\n",
      "Epoch   0 Batch  670/11516   train_loss = 4.354\n",
      "Epoch   0 Batch  671/11516   train_loss = 8.875\n",
      "Epoch   0 Batch  672/11516   train_loss = 9.370\n",
      "Epoch   0 Batch  673/11516   train_loss = 6.393\n",
      "Epoch   0 Batch  674/11516   train_loss = 6.172\n",
      "Epoch   0 Batch  675/11516   train_loss = 6.309\n",
      "Epoch   0 Batch  676/11516   train_loss = 5.770\n",
      "Epoch   0 Batch  677/11516   train_loss = 5.510\n",
      "Epoch   0 Batch  678/11516   train_loss = 3.068\n",
      "Epoch   0 Batch  679/11516   train_loss = 3.844\n",
      "Epoch   0 Batch  680/11516   train_loss = 5.671\n",
      "Epoch   0 Batch  681/11516   train_loss = 5.411\n",
      "Epoch   0 Batch  682/11516   train_loss = 5.021\n",
      "Epoch   0 Batch  683/11516   train_loss = 6.795\n",
      "Epoch   0 Batch  684/11516   train_loss = 5.988\n",
      "Epoch   0 Batch  685/11516   train_loss = 5.648\n",
      "Epoch   0 Batch  686/11516   train_loss = 6.194\n",
      "Epoch   0 Batch  687/11516   train_loss = 7.178\n",
      "Epoch   0 Batch  688/11516   train_loss = 5.411\n",
      "Epoch   0 Batch  689/11516   train_loss = 3.786\n",
      "Epoch   0 Batch  690/11516   train_loss = 7.118\n",
      "Epoch   0 Batch  691/11516   train_loss = 8.522\n",
      "Epoch   0 Batch  692/11516   train_loss = 7.762\n",
      "Epoch   0 Batch  693/11516   train_loss = 6.796\n",
      "Epoch   0 Batch  694/11516   train_loss = 7.873\n",
      "Epoch   0 Batch  695/11516   train_loss = 2.917\n",
      "Epoch   0 Batch  696/11516   train_loss = 5.680\n",
      "Epoch   0 Batch  697/11516   train_loss = 4.500\n",
      "Epoch   0 Batch  698/11516   train_loss = 5.800\n",
      "Epoch   0 Batch  699/11516   train_loss = 4.134\n",
      "Epoch   0 Batch  700/11516   train_loss = 5.054\n",
      "Epoch   0 Batch  701/11516   train_loss = 3.614\n",
      "Epoch   0 Batch  702/11516   train_loss = 5.779\n",
      "Epoch   0 Batch  703/11516   train_loss = 7.299\n",
      "Epoch   0 Batch  704/11516   train_loss = 7.047\n",
      "Epoch   0 Batch  705/11516   train_loss = 4.592\n",
      "Epoch   0 Batch  706/11516   train_loss = 5.773\n",
      "Epoch   0 Batch  707/11516   train_loss = 4.710\n",
      "Epoch   0 Batch  708/11516   train_loss = 4.309\n",
      "Epoch   0 Batch  709/11516   train_loss = 6.054\n",
      "Epoch   0 Batch  710/11516   train_loss = 6.318\n",
      "Epoch   0 Batch  711/11516   train_loss = 2.990\n",
      "Epoch   0 Batch  712/11516   train_loss = 6.434\n",
      "Epoch   0 Batch  713/11516   train_loss = 5.384\n",
      "Epoch   0 Batch  714/11516   train_loss = 5.548\n",
      "Epoch   0 Batch  715/11516   train_loss = 6.136\n",
      "Epoch   0 Batch  716/11516   train_loss = 5.823\n",
      "Epoch   0 Batch  717/11516   train_loss = 6.019\n",
      "Epoch   0 Batch  718/11516   train_loss = 8.631\n",
      "Epoch   0 Batch  719/11516   train_loss = 8.675\n",
      "Epoch   0 Batch  720/11516   train_loss = 3.625\n",
      "Epoch   0 Batch  721/11516   train_loss = 7.613\n",
      "Epoch   0 Batch  722/11516   train_loss = 6.042\n",
      "Epoch   0 Batch  723/11516   train_loss = 8.758\n",
      "Epoch   0 Batch  724/11516   train_loss = 5.518\n",
      "Epoch   0 Batch  725/11516   train_loss = 7.549\n",
      "Epoch   0 Batch  726/11516   train_loss = 5.738\n",
      "Epoch   0 Batch  727/11516   train_loss = 3.829\n",
      "Epoch   0 Batch  728/11516   train_loss = 4.889\n",
      "Epoch   0 Batch  729/11516   train_loss = 5.500\n",
      "Epoch   0 Batch  730/11516   train_loss = 7.220\n",
      "Epoch   0 Batch  731/11516   train_loss = 5.430\n",
      "Epoch   0 Batch  732/11516   train_loss = 5.775\n",
      "Epoch   0 Batch  733/11516   train_loss = 3.725\n",
      "Epoch   0 Batch  734/11516   train_loss = 8.360\n",
      "Epoch   0 Batch  735/11516   train_loss = 5.057\n",
      "Epoch   0 Batch  736/11516   train_loss = 7.176\n",
      "Epoch   0 Batch  737/11516   train_loss = 6.612\n",
      "Epoch   0 Batch  738/11516   train_loss = 6.362\n",
      "Epoch   0 Batch  739/11516   train_loss = 7.451\n",
      "Epoch   0 Batch  740/11516   train_loss = 7.585\n",
      "Epoch   0 Batch  741/11516   train_loss = 8.116\n",
      "Epoch   0 Batch  742/11516   train_loss = 6.964\n",
      "Epoch   0 Batch  743/11516   train_loss = 5.679\n",
      "Epoch   0 Batch  744/11516   train_loss = 4.446\n",
      "Epoch   0 Batch  745/11516   train_loss = 7.122\n",
      "Epoch   0 Batch  746/11516   train_loss = 3.615\n",
      "Epoch   0 Batch  747/11516   train_loss = 9.906\n",
      "Epoch   0 Batch  748/11516   train_loss = 6.681\n",
      "Epoch   0 Batch  749/11516   train_loss = 6.929\n",
      "Epoch   0 Batch  750/11516   train_loss = 7.574\n",
      "Epoch   0 Batch  751/11516   train_loss = 8.975\n",
      "Epoch   0 Batch  752/11516   train_loss = 9.808\n",
      "Epoch   0 Batch  753/11516   train_loss = 4.543\n",
      "Epoch   0 Batch  754/11516   train_loss = 6.113\n",
      "Epoch   0 Batch  755/11516   train_loss = 7.954\n",
      "Epoch   0 Batch  756/11516   train_loss = 8.522\n",
      "Epoch   0 Batch  757/11516   train_loss = 8.883\n",
      "Epoch   0 Batch  758/11516   train_loss = 7.830\n",
      "Epoch   0 Batch  759/11516   train_loss = 6.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  760/11516   train_loss = 6.096\n",
      "Epoch   0 Batch  761/11516   train_loss = 2.863\n",
      "Epoch   0 Batch  762/11516   train_loss = 4.674\n",
      "Epoch   0 Batch  763/11516   train_loss = 5.107\n",
      "Epoch   0 Batch  764/11516   train_loss = 5.753\n",
      "Epoch   0 Batch  765/11516   train_loss = 7.025\n",
      "Epoch   0 Batch  766/11516   train_loss = 7.338\n",
      "Epoch   0 Batch  767/11516   train_loss = 8.671\n",
      "Epoch   0 Batch  768/11516   train_loss = 4.186\n",
      "Epoch   0 Batch  769/11516   train_loss = 6.330\n",
      "Epoch   0 Batch  770/11516   train_loss = 5.725\n",
      "Epoch   0 Batch  771/11516   train_loss = 7.190\n",
      "Epoch   0 Batch  772/11516   train_loss = 8.593\n",
      "Epoch   0 Batch  773/11516   train_loss = 6.148\n",
      "Epoch   0 Batch  774/11516   train_loss = 3.927\n",
      "Epoch   0 Batch  775/11516   train_loss = 6.432\n",
      "Epoch   0 Batch  776/11516   train_loss = 4.221\n",
      "Epoch   0 Batch  777/11516   train_loss = 7.113\n",
      "Epoch   0 Batch  778/11516   train_loss = 8.017\n",
      "Epoch   0 Batch  779/11516   train_loss = 6.905\n",
      "Epoch   0 Batch  780/11516   train_loss = 4.269\n",
      "Epoch   0 Batch  781/11516   train_loss = 3.630\n",
      "Epoch   0 Batch  782/11516   train_loss = 6.141\n",
      "Epoch   0 Batch  783/11516   train_loss = 5.438\n",
      "Epoch   0 Batch  784/11516   train_loss = 6.614\n",
      "Epoch   0 Batch  785/11516   train_loss = 7.945\n",
      "Epoch   0 Batch  786/11516   train_loss = 6.896\n",
      "Epoch   0 Batch  787/11516   train_loss = 8.972\n",
      "Epoch   0 Batch  788/11516   train_loss = 10.581\n",
      "Epoch   0 Batch  789/11516   train_loss = 7.861\n",
      "Epoch   0 Batch  790/11516   train_loss = 2.961\n",
      "Epoch   0 Batch  791/11516   train_loss = 3.903\n",
      "Epoch   0 Batch  792/11516   train_loss = 5.432\n",
      "Epoch   0 Batch  793/11516   train_loss = 6.586\n",
      "Epoch   0 Batch  794/11516   train_loss = 4.995\n",
      "Epoch   0 Batch  795/11516   train_loss = 5.836\n",
      "Epoch   0 Batch  796/11516   train_loss = 5.259\n",
      "Epoch   0 Batch  797/11516   train_loss = 5.800\n",
      "Epoch   0 Batch  798/11516   train_loss = 9.176\n",
      "Epoch   0 Batch  799/11516   train_loss = 6.862\n",
      "Epoch   0 Batch  800/11516   train_loss = 9.877\n",
      "Epoch   0 Batch  801/11516   train_loss = 8.554\n",
      "Epoch   0 Batch  802/11516   train_loss = 2.297\n",
      "Epoch   0 Batch  803/11516   train_loss = 5.798\n",
      "Epoch   0 Batch  804/11516   train_loss = 5.615\n",
      "Epoch   0 Batch  805/11516   train_loss = 7.657\n",
      "Epoch   0 Batch  806/11516   train_loss = 8.762\n",
      "Epoch   0 Batch  807/11516   train_loss = 7.143\n",
      "Epoch   0 Batch  808/11516   train_loss = 4.688\n",
      "Epoch   0 Batch  809/11516   train_loss = 5.814\n",
      "Epoch   0 Batch  810/11516   train_loss = 5.380\n",
      "Epoch   0 Batch  811/11516   train_loss = 6.233\n",
      "Epoch   0 Batch  812/11516   train_loss = 8.032\n",
      "Epoch   0 Batch  813/11516   train_loss = 7.518\n",
      "Epoch   0 Batch  814/11516   train_loss = 7.235\n",
      "Epoch   0 Batch  815/11516   train_loss = 4.262\n",
      "Epoch   0 Batch  816/11516   train_loss = 5.753\n",
      "Epoch   0 Batch  817/11516   train_loss = 4.513\n",
      "Epoch   0 Batch  818/11516   train_loss = 6.972\n",
      "Epoch   0 Batch  819/11516   train_loss = 7.327\n",
      "Epoch   0 Batch  820/11516   train_loss = 7.410\n",
      "Epoch   0 Batch  821/11516   train_loss = 4.681\n",
      "Epoch   0 Batch  822/11516   train_loss = 5.550\n",
      "Epoch   0 Batch  823/11516   train_loss = 5.938\n",
      "Epoch   0 Batch  824/11516   train_loss = 5.289\n",
      "Epoch   0 Batch  825/11516   train_loss = 7.692\n",
      "Epoch   0 Batch  826/11516   train_loss = 8.611\n",
      "Epoch   0 Batch  827/11516   train_loss = 8.679\n",
      "Epoch   0 Batch  828/11516   train_loss = 6.123\n",
      "Epoch   0 Batch  829/11516   train_loss = 4.281\n",
      "Epoch   0 Batch  830/11516   train_loss = 7.917\n",
      "Epoch   0 Batch  831/11516   train_loss = 4.889\n",
      "Epoch   0 Batch  832/11516   train_loss = 5.715\n",
      "Epoch   0 Batch  833/11516   train_loss = 4.790\n",
      "Epoch   0 Batch  834/11516   train_loss = 6.968\n",
      "Epoch   0 Batch  835/11516   train_loss = 5.149\n",
      "Epoch   0 Batch  836/11516   train_loss = 3.496\n",
      "Epoch   0 Batch  837/11516   train_loss = 7.150\n",
      "Epoch   0 Batch  838/11516   train_loss = 9.109\n",
      "Epoch   0 Batch  839/11516   train_loss = 5.040\n",
      "Epoch   0 Batch  840/11516   train_loss = 8.592\n",
      "Epoch   0 Batch  841/11516   train_loss = 6.246\n",
      "Epoch   0 Batch  842/11516   train_loss = 4.908\n",
      "Epoch   0 Batch  843/11516   train_loss = 8.267\n",
      "Epoch   0 Batch  844/11516   train_loss = 8.894\n",
      "Epoch   0 Batch  845/11516   train_loss = 7.843\n",
      "Epoch   0 Batch  846/11516   train_loss = 6.693\n",
      "Epoch   0 Batch  847/11516   train_loss = 5.816\n",
      "Epoch   0 Batch  848/11516   train_loss = 6.897\n",
      "Epoch   0 Batch  849/11516   train_loss = 9.898\n",
      "Epoch   0 Batch  850/11516   train_loss = 5.105\n",
      "Epoch   0 Batch  851/11516   train_loss = 5.550\n",
      "Epoch   0 Batch  852/11516   train_loss = 7.468\n",
      "Epoch   0 Batch  853/11516   train_loss = 9.526\n",
      "Epoch   0 Batch  854/11516   train_loss = 6.438\n",
      "Epoch   0 Batch  855/11516   train_loss = 5.644\n",
      "Epoch   0 Batch  856/11516   train_loss = 6.919\n",
      "Epoch   0 Batch  857/11516   train_loss = 6.610\n",
      "Epoch   0 Batch  858/11516   train_loss = 11.587\n",
      "Epoch   0 Batch  859/11516   train_loss = 4.344\n",
      "Epoch   0 Batch  860/11516   train_loss = 3.816\n",
      "Epoch   0 Batch  861/11516   train_loss = 5.408\n",
      "Epoch   0 Batch  862/11516   train_loss = 8.705\n",
      "Epoch   0 Batch  863/11516   train_loss = 6.859\n",
      "Epoch   0 Batch  864/11516   train_loss = 4.163\n",
      "Epoch   0 Batch  865/11516   train_loss = 4.759\n",
      "Epoch   0 Batch  866/11516   train_loss = 5.313\n",
      "Epoch   0 Batch  867/11516   train_loss = 9.595\n",
      "Epoch   0 Batch  868/11516   train_loss = 7.083\n",
      "Epoch   0 Batch  869/11516   train_loss = 3.985\n",
      "Epoch   0 Batch  870/11516   train_loss = 5.409\n",
      "Epoch   0 Batch  871/11516   train_loss = 6.643\n",
      "Epoch   0 Batch  872/11516   train_loss = 5.339\n",
      "Epoch   0 Batch  873/11516   train_loss = 7.065\n",
      "Epoch   0 Batch  874/11516   train_loss = 6.672\n",
      "Epoch   0 Batch  875/11516   train_loss = 9.689\n",
      "Epoch   0 Batch  876/11516   train_loss = 5.232\n",
      "Epoch   0 Batch  877/11516   train_loss = 5.789\n",
      "Epoch   0 Batch  878/11516   train_loss = 5.893\n",
      "Epoch   0 Batch  879/11516   train_loss = 7.709\n",
      "Epoch   0 Batch  880/11516   train_loss = 7.737\n",
      "Epoch   0 Batch  881/11516   train_loss = 5.711\n",
      "Epoch   0 Batch  882/11516   train_loss = 5.210\n",
      "Epoch   0 Batch  883/11516   train_loss = 4.599\n",
      "Epoch   0 Batch  884/11516   train_loss = 6.260\n",
      "Epoch   0 Batch  885/11516   train_loss = 3.658\n",
      "Epoch   0 Batch  886/11516   train_loss = 5.553\n",
      "Epoch   0 Batch  887/11516   train_loss = 9.481\n",
      "Epoch   0 Batch  888/11516   train_loss = 6.211\n",
      "Epoch   0 Batch  889/11516   train_loss = 6.258\n",
      "Epoch   0 Batch  890/11516   train_loss = 6.692\n",
      "Epoch   0 Batch  891/11516   train_loss = 7.618\n",
      "Epoch   0 Batch  892/11516   train_loss = 5.133\n",
      "Epoch   0 Batch  893/11516   train_loss = 5.137\n",
      "Epoch   0 Batch  894/11516   train_loss = 5.347\n",
      "Epoch   0 Batch  895/11516   train_loss = 7.928\n",
      "Epoch   0 Batch  896/11516   train_loss = 6.369\n",
      "Epoch   0 Batch  897/11516   train_loss = 5.678\n",
      "Epoch   0 Batch  898/11516   train_loss = 5.119\n",
      "Epoch   0 Batch  899/11516   train_loss = 7.308\n",
      "Epoch   0 Batch  900/11516   train_loss = 6.078\n",
      "Epoch   0 Batch  901/11516   train_loss = 6.436\n",
      "Epoch   0 Batch  902/11516   train_loss = 6.613\n",
      "Epoch   0 Batch  903/11516   train_loss = 4.964\n",
      "Epoch   0 Batch  904/11516   train_loss = 3.247\n",
      "Epoch   0 Batch  905/11516   train_loss = 5.800\n",
      "Epoch   0 Batch  906/11516   train_loss = 6.218\n",
      "Epoch   0 Batch  907/11516   train_loss = 4.566\n",
      "Epoch   0 Batch  908/11516   train_loss = 7.033\n",
      "Epoch   0 Batch  909/11516   train_loss = 7.211\n",
      "Epoch   0 Batch  910/11516   train_loss = 8.793\n",
      "Epoch   0 Batch  911/11516   train_loss = 4.791\n",
      "Epoch   0 Batch  912/11516   train_loss = 5.507\n",
      "Epoch   0 Batch  913/11516   train_loss = 6.069\n",
      "Epoch   0 Batch  914/11516   train_loss = 9.733\n",
      "Epoch   0 Batch  915/11516   train_loss = 5.963\n",
      "Epoch   0 Batch  916/11516   train_loss = 3.171\n",
      "Epoch   0 Batch  917/11516   train_loss = 5.909\n",
      "Epoch   0 Batch  918/11516   train_loss = 7.435\n",
      "Epoch   0 Batch  919/11516   train_loss = 8.315\n",
      "Epoch   0 Batch  920/11516   train_loss = 5.133\n",
      "Epoch   0 Batch  921/11516   train_loss = 5.117\n",
      "Epoch   0 Batch  922/11516   train_loss = 5.389\n",
      "Epoch   0 Batch  923/11516   train_loss = 7.246\n",
      "Epoch   0 Batch  924/11516   train_loss = 3.194\n",
      "Epoch   0 Batch  925/11516   train_loss = 5.152\n",
      "Epoch   0 Batch  926/11516   train_loss = 6.327\n",
      "Epoch   0 Batch  927/11516   train_loss = 9.482\n",
      "Epoch   0 Batch  928/11516   train_loss = 3.984\n",
      "Epoch   0 Batch  929/11516   train_loss = 5.179\n",
      "Epoch   0 Batch  930/11516   train_loss = 8.058\n",
      "Epoch   0 Batch  931/11516   train_loss = 6.323\n",
      "Epoch   0 Batch  932/11516   train_loss = 7.260\n",
      "Epoch   0 Batch  933/11516   train_loss = 7.260\n",
      "Epoch   0 Batch  934/11516   train_loss = 6.786\n",
      "Epoch   0 Batch  935/11516   train_loss = 8.736\n",
      "Epoch   0 Batch  936/11516   train_loss = 4.395\n",
      "Epoch   0 Batch  937/11516   train_loss = 7.685\n",
      "Epoch   0 Batch  938/11516   train_loss = 5.486\n",
      "Epoch   0 Batch  939/11516   train_loss = 7.137\n",
      "Epoch   0 Batch  940/11516   train_loss = 11.012\n",
      "Epoch   0 Batch  941/11516   train_loss = 2.680\n",
      "Epoch   0 Batch  942/11516   train_loss = 6.951\n",
      "Epoch   0 Batch  943/11516   train_loss = 4.504\n",
      "Epoch   0 Batch  944/11516   train_loss = 6.860\n",
      "Epoch   0 Batch  945/11516   train_loss = 6.706\n",
      "Epoch   0 Batch  946/11516   train_loss = 9.741\n",
      "Epoch   0 Batch  947/11516   train_loss = 4.507\n",
      "Epoch   0 Batch  948/11516   train_loss = 9.827\n",
      "Epoch   0 Batch  949/11516   train_loss = 8.187\n",
      "Epoch   0 Batch  950/11516   train_loss = 5.327\n",
      "Epoch   0 Batch  951/11516   train_loss = 7.232\n",
      "Epoch   0 Batch  952/11516   train_loss = 6.070\n",
      "Epoch   0 Batch  953/11516   train_loss = 4.422\n",
      "Epoch   0 Batch  954/11516   train_loss = 9.071\n",
      "Epoch   0 Batch  955/11516   train_loss = 3.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  956/11516   train_loss = 7.339\n",
      "Epoch   0 Batch  957/11516   train_loss = 4.697\n",
      "Epoch   0 Batch  958/11516   train_loss = 4.291\n",
      "Epoch   0 Batch  959/11516   train_loss = 6.461\n",
      "Epoch   0 Batch  960/11516   train_loss = 4.196\n",
      "Epoch   0 Batch  961/11516   train_loss = 4.967\n",
      "Epoch   0 Batch  962/11516   train_loss = 8.033\n",
      "Epoch   0 Batch  963/11516   train_loss = 6.597\n",
      "Epoch   0 Batch  964/11516   train_loss = 7.746\n",
      "Epoch   0 Batch  965/11516   train_loss = 6.926\n",
      "Epoch   0 Batch  966/11516   train_loss = 5.521\n",
      "Epoch   0 Batch  967/11516   train_loss = 3.815\n",
      "Epoch   0 Batch  968/11516   train_loss = 5.739\n",
      "Epoch   0 Batch  969/11516   train_loss = 7.546\n",
      "Epoch   0 Batch  970/11516   train_loss = 5.860\n",
      "Epoch   0 Batch  971/11516   train_loss = 7.260\n",
      "Epoch   0 Batch  972/11516   train_loss = 6.696\n",
      "Epoch   0 Batch  973/11516   train_loss = 6.536\n",
      "Epoch   0 Batch  974/11516   train_loss = 6.175\n",
      "Epoch   0 Batch  975/11516   train_loss = 6.308\n",
      "Epoch   0 Batch  976/11516   train_loss = 9.758\n",
      "Epoch   0 Batch  977/11516   train_loss = 6.304\n",
      "Epoch   0 Batch  978/11516   train_loss = 6.020\n",
      "Epoch   0 Batch  979/11516   train_loss = 4.370\n",
      "Epoch   0 Batch  980/11516   train_loss = 7.828\n",
      "Epoch   0 Batch  981/11516   train_loss = 5.191\n",
      "Epoch   0 Batch  982/11516   train_loss = 7.052\n",
      "Epoch   0 Batch  983/11516   train_loss = 4.740\n",
      "Epoch   0 Batch  984/11516   train_loss = 8.082\n",
      "Epoch   0 Batch  985/11516   train_loss = 7.807\n",
      "Epoch   0 Batch  986/11516   train_loss = 5.995\n",
      "Epoch   0 Batch  987/11516   train_loss = 7.311\n",
      "Epoch   0 Batch  988/11516   train_loss = 3.744\n",
      "Epoch   0 Batch  989/11516   train_loss = 3.965\n",
      "Epoch   0 Batch  990/11516   train_loss = 9.126\n",
      "Epoch   0 Batch  991/11516   train_loss = 4.666\n",
      "Epoch   0 Batch  992/11516   train_loss = 5.058\n",
      "Epoch   0 Batch  993/11516   train_loss = 6.395\n",
      "Epoch   0 Batch  994/11516   train_loss = 6.089\n",
      "Epoch   0 Batch  995/11516   train_loss = 4.683\n",
      "Epoch   0 Batch  996/11516   train_loss = 7.168\n",
      "Epoch   0 Batch  997/11516   train_loss = 8.216\n",
      "Epoch   0 Batch  998/11516   train_loss = 5.787\n",
      "Epoch   0 Batch  999/11516   train_loss = 6.168\n",
      "Epoch   0 Batch 1000/11516   train_loss = 7.023\n",
      "Epoch   0 Batch 1001/11516   train_loss = 6.643\n",
      "Epoch   0 Batch 1002/11516   train_loss = 6.945\n",
      "Epoch   0 Batch 1003/11516   train_loss = 6.361\n",
      "Epoch   0 Batch 1004/11516   train_loss = 5.409\n",
      "Epoch   0 Batch 1005/11516   train_loss = 9.503\n",
      "Epoch   0 Batch 1006/11516   train_loss = 4.665\n",
      "Epoch   0 Batch 1007/11516   train_loss = 5.117\n",
      "Epoch   0 Batch 1008/11516   train_loss = 5.999\n",
      "Epoch   0 Batch 1009/11516   train_loss = 7.646\n",
      "Epoch   0 Batch 1010/11516   train_loss = 5.920\n",
      "Epoch   0 Batch 1011/11516   train_loss = 4.436\n",
      "Epoch   0 Batch 1012/11516   train_loss = 6.066\n",
      "Epoch   0 Batch 1013/11516   train_loss = 5.615\n",
      "Epoch   0 Batch 1014/11516   train_loss = 5.985\n",
      "Epoch   0 Batch 1015/11516   train_loss = 9.181\n",
      "Epoch   0 Batch 1016/11516   train_loss = 5.073\n",
      "Epoch   0 Batch 1017/11516   train_loss = 8.292\n",
      "Epoch   0 Batch 1018/11516   train_loss = 5.242\n",
      "Epoch   0 Batch 1019/11516   train_loss = 7.654\n",
      "Epoch   0 Batch 1020/11516   train_loss = 4.463\n",
      "Epoch   0 Batch 1021/11516   train_loss = 5.728\n",
      "Epoch   0 Batch 1022/11516   train_loss = 6.375\n",
      "Epoch   0 Batch 1023/11516   train_loss = 3.524\n",
      "Epoch   0 Batch 1024/11516   train_loss = 5.943\n",
      "Epoch   0 Batch 1025/11516   train_loss = 9.180\n",
      "Epoch   0 Batch 1026/11516   train_loss = 6.503\n",
      "Epoch   0 Batch 1027/11516   train_loss = 9.275\n",
      "Epoch   0 Batch 1028/11516   train_loss = 3.369\n",
      "Epoch   0 Batch 1029/11516   train_loss = 6.085\n",
      "Epoch   0 Batch 1030/11516   train_loss = 7.950\n",
      "Epoch   0 Batch 1031/11516   train_loss = 4.418\n",
      "Epoch   0 Batch 1032/11516   train_loss = 5.891\n",
      "Epoch   0 Batch 1033/11516   train_loss = 6.142\n",
      "Epoch   0 Batch 1034/11516   train_loss = 6.886\n",
      "Epoch   0 Batch 1035/11516   train_loss = 5.330\n",
      "Epoch   0 Batch 1036/11516   train_loss = 9.776\n",
      "Epoch   0 Batch 1037/11516   train_loss = 6.204\n",
      "Epoch   0 Batch 1038/11516   train_loss = 5.195\n",
      "Epoch   0 Batch 1039/11516   train_loss = 6.404\n",
      "Epoch   0 Batch 1040/11516   train_loss = 5.840\n",
      "Epoch   0 Batch 1041/11516   train_loss = 5.608\n",
      "Epoch   0 Batch 1042/11516   train_loss = 6.425\n",
      "Epoch   0 Batch 1043/11516   train_loss = 8.158\n",
      "Epoch   0 Batch 1044/11516   train_loss = 8.144\n",
      "Epoch   0 Batch 1045/11516   train_loss = 4.539\n",
      "Epoch   0 Batch 1046/11516   train_loss = 5.703\n",
      "Epoch   0 Batch 1047/11516   train_loss = 5.619\n",
      "Epoch   0 Batch 1048/11516   train_loss = 4.788\n",
      "Epoch   0 Batch 1049/11516   train_loss = 5.525\n",
      "Epoch   0 Batch 1050/11516   train_loss = 5.262\n",
      "Epoch   0 Batch 1051/11516   train_loss = 6.314\n",
      "Epoch   0 Batch 1052/11516   train_loss = 3.589\n",
      "Epoch   0 Batch 1053/11516   train_loss = 8.275\n",
      "Epoch   0 Batch 1054/11516   train_loss = 7.334\n",
      "Epoch   0 Batch 1055/11516   train_loss = 4.654\n",
      "Epoch   0 Batch 1056/11516   train_loss = 6.735\n",
      "Epoch   0 Batch 1057/11516   train_loss = 5.469\n",
      "Epoch   0 Batch 1058/11516   train_loss = 5.751\n",
      "Epoch   0 Batch 1059/11516   train_loss = 3.713\n",
      "Epoch   0 Batch 1060/11516   train_loss = 4.427\n",
      "Epoch   0 Batch 1061/11516   train_loss = 7.734\n",
      "Epoch   0 Batch 1062/11516   train_loss = 8.288\n",
      "Epoch   0 Batch 1063/11516   train_loss = 7.369\n",
      "Epoch   0 Batch 1064/11516   train_loss = 3.727\n",
      "Epoch   0 Batch 1065/11516   train_loss = 2.936\n",
      "Epoch   0 Batch 1066/11516   train_loss = 6.570\n",
      "Epoch   0 Batch 1067/11516   train_loss = 6.652\n",
      "Epoch   0 Batch 1068/11516   train_loss = 3.332\n",
      "Epoch   0 Batch 1069/11516   train_loss = 3.974\n",
      "Epoch   0 Batch 1070/11516   train_loss = 7.090\n",
      "Epoch   0 Batch 1071/11516   train_loss = 4.308\n",
      "Epoch   0 Batch 1072/11516   train_loss = 6.359\n",
      "Epoch   0 Batch 1073/11516   train_loss = 6.760\n",
      "Epoch   0 Batch 1074/11516   train_loss = 6.442\n",
      "Epoch   0 Batch 1075/11516   train_loss = 8.374\n",
      "Epoch   0 Batch 1076/11516   train_loss = 5.443\n",
      "Epoch   0 Batch 1077/11516   train_loss = 6.711\n",
      "Epoch   0 Batch 1078/11516   train_loss = 2.996\n",
      "Epoch   0 Batch 1079/11516   train_loss = 4.797\n",
      "Epoch   0 Batch 1080/11516   train_loss = 5.060\n",
      "Epoch   0 Batch 1081/11516   train_loss = 5.681\n",
      "Epoch   0 Batch 1082/11516   train_loss = 4.300\n",
      "Epoch   0 Batch 1083/11516   train_loss = 5.782\n",
      "Epoch   0 Batch 1084/11516   train_loss = 8.579\n",
      "Epoch   0 Batch 1085/11516   train_loss = 3.281\n",
      "Epoch   0 Batch 1086/11516   train_loss = 6.306\n",
      "Epoch   0 Batch 1087/11516   train_loss = 6.748\n",
      "Epoch   0 Batch 1088/11516   train_loss = 8.816\n",
      "Epoch   0 Batch 1089/11516   train_loss = 6.963\n",
      "Epoch   0 Batch 1090/11516   train_loss = 11.545\n",
      "Epoch   0 Batch 1091/11516   train_loss = 6.707\n",
      "Epoch   0 Batch 1092/11516   train_loss = 5.759\n",
      "Epoch   0 Batch 1093/11516   train_loss = 4.985\n",
      "Epoch   0 Batch 1094/11516   train_loss = 5.822\n",
      "Epoch   0 Batch 1095/11516   train_loss = 4.406\n",
      "Epoch   0 Batch 1096/11516   train_loss = 7.237\n",
      "Epoch   0 Batch 1097/11516   train_loss = 5.862\n",
      "Epoch   0 Batch 1098/11516   train_loss = 5.571\n",
      "Epoch   0 Batch 1099/11516   train_loss = 6.483\n",
      "Epoch   0 Batch 1100/11516   train_loss = 5.736\n",
      "Epoch   0 Batch 1101/11516   train_loss = 7.721\n",
      "Epoch   0 Batch 1102/11516   train_loss = 7.491\n",
      "Epoch   0 Batch 1103/11516   train_loss = 8.158\n",
      "Epoch   0 Batch 1104/11516   train_loss = 9.934\n",
      "Epoch   0 Batch 1105/11516   train_loss = 6.571\n",
      "Epoch   0 Batch 1106/11516   train_loss = 6.758\n",
      "Epoch   0 Batch 1107/11516   train_loss = 7.611\n",
      "Epoch   0 Batch 1108/11516   train_loss = 6.594\n",
      "Epoch   0 Batch 1109/11516   train_loss = 7.464\n",
      "Epoch   0 Batch 1110/11516   train_loss = 7.258\n",
      "Epoch   0 Batch 1111/11516   train_loss = 5.707\n",
      "Epoch   0 Batch 1112/11516   train_loss = 9.632\n",
      "Epoch   0 Batch 1113/11516   train_loss = 5.873\n",
      "Epoch   0 Batch 1114/11516   train_loss = 3.208\n",
      "Epoch   0 Batch 1115/11516   train_loss = 3.598\n",
      "Epoch   0 Batch 1116/11516   train_loss = 4.777\n",
      "Epoch   0 Batch 1117/11516   train_loss = 7.788\n",
      "Epoch   0 Batch 1118/11516   train_loss = 3.950\n",
      "Epoch   0 Batch 1119/11516   train_loss = 7.013\n",
      "Epoch   0 Batch 1120/11516   train_loss = 11.424\n",
      "Epoch   0 Batch 1121/11516   train_loss = 9.933\n",
      "Epoch   0 Batch 1122/11516   train_loss = 4.633\n",
      "Epoch   0 Batch 1123/11516   train_loss = 6.683\n",
      "Epoch   0 Batch 1124/11516   train_loss = 3.500\n",
      "Epoch   0 Batch 1125/11516   train_loss = 8.060\n",
      "Epoch   0 Batch 1126/11516   train_loss = 6.059\n",
      "Epoch   0 Batch 1127/11516   train_loss = 5.297\n",
      "Epoch   0 Batch 1128/11516   train_loss = 4.176\n",
      "Epoch   0 Batch 1129/11516   train_loss = 3.967\n",
      "Epoch   0 Batch 1130/11516   train_loss = 4.623\n",
      "Epoch   0 Batch 1131/11516   train_loss = 6.579\n",
      "Epoch   0 Batch 1132/11516   train_loss = 7.723\n",
      "Epoch   0 Batch 1133/11516   train_loss = 7.958\n",
      "Epoch   0 Batch 1134/11516   train_loss = 8.280\n",
      "Epoch   0 Batch 1135/11516   train_loss = 6.460\n",
      "Epoch   0 Batch 1136/11516   train_loss = 7.809\n",
      "Epoch   0 Batch 1137/11516   train_loss = 4.501\n",
      "Epoch   0 Batch 1138/11516   train_loss = 5.849\n",
      "Epoch   0 Batch 1139/11516   train_loss = 7.157\n",
      "Epoch   0 Batch 1140/11516   train_loss = 6.793\n",
      "Epoch   0 Batch 1141/11516   train_loss = 8.449\n",
      "Epoch   0 Batch 1142/11516   train_loss = 4.856\n",
      "Epoch   0 Batch 1143/11516   train_loss = 4.951\n",
      "Epoch   0 Batch 1144/11516   train_loss = 7.223\n",
      "Epoch   0 Batch 1145/11516   train_loss = 4.355\n",
      "Epoch   0 Batch 1146/11516   train_loss = 5.785\n",
      "Epoch   0 Batch 1147/11516   train_loss = 3.927\n",
      "Epoch   0 Batch 1148/11516   train_loss = 7.014\n",
      "Epoch   0 Batch 1149/11516   train_loss = 6.889\n",
      "Epoch   0 Batch 1150/11516   train_loss = 5.045\n",
      "Epoch   0 Batch 1151/11516   train_loss = 3.706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1152/11516   train_loss = 7.134\n",
      "Epoch   0 Batch 1153/11516   train_loss = 7.895\n",
      "Epoch   0 Batch 1154/11516   train_loss = 11.253\n",
      "Epoch   0 Batch 1155/11516   train_loss = 6.725\n",
      "Epoch   0 Batch 1156/11516   train_loss = 3.270\n",
      "Epoch   0 Batch 1157/11516   train_loss = 7.870\n",
      "Epoch   0 Batch 1158/11516   train_loss = 8.478\n",
      "Epoch   0 Batch 1159/11516   train_loss = 5.867\n",
      "Epoch   0 Batch 1160/11516   train_loss = 6.970\n",
      "Epoch   0 Batch 1161/11516   train_loss = 7.894\n",
      "Epoch   0 Batch 1162/11516   train_loss = 9.640\n",
      "Epoch   0 Batch 1163/11516   train_loss = 7.407\n",
      "Epoch   0 Batch 1164/11516   train_loss = 6.690\n",
      "Epoch   0 Batch 1165/11516   train_loss = 8.087\n",
      "Epoch   0 Batch 1166/11516   train_loss = 6.455\n",
      "Epoch   0 Batch 1167/11516   train_loss = 6.458\n",
      "Epoch   0 Batch 1168/11516   train_loss = 7.240\n",
      "Epoch   0 Batch 1169/11516   train_loss = 7.972\n",
      "Epoch   0 Batch 1170/11516   train_loss = 4.760\n",
      "Epoch   0 Batch 1171/11516   train_loss = 8.110\n",
      "Epoch   0 Batch 1172/11516   train_loss = 5.803\n",
      "Epoch   0 Batch 1173/11516   train_loss = 6.162\n",
      "Epoch   0 Batch 1174/11516   train_loss = 6.164\n",
      "Epoch   0 Batch 1175/11516   train_loss = 6.264\n",
      "Epoch   0 Batch 1176/11516   train_loss = 7.387\n",
      "Epoch   0 Batch 1177/11516   train_loss = 6.505\n",
      "Epoch   0 Batch 1178/11516   train_loss = 5.035\n",
      "Epoch   0 Batch 1179/11516   train_loss = 7.134\n",
      "Epoch   0 Batch 1180/11516   train_loss = 7.783\n",
      "Epoch   0 Batch 1181/11516   train_loss = 7.702\n",
      "Epoch   0 Batch 1182/11516   train_loss = 10.640\n",
      "Epoch   0 Batch 1183/11516   train_loss = 7.064\n",
      "Epoch   0 Batch 1184/11516   train_loss = 8.768\n",
      "Epoch   0 Batch 1185/11516   train_loss = 6.104\n",
      "Epoch   0 Batch 1186/11516   train_loss = 6.607\n",
      "Epoch   0 Batch 1187/11516   train_loss = 6.327\n",
      "Epoch   0 Batch 1188/11516   train_loss = 6.061\n",
      "Epoch   0 Batch 1189/11516   train_loss = 8.685\n",
      "Epoch   0 Batch 1190/11516   train_loss = 5.352\n",
      "Epoch   0 Batch 1191/11516   train_loss = 5.303\n",
      "Epoch   0 Batch 1192/11516   train_loss = 7.206\n",
      "Epoch   0 Batch 1193/11516   train_loss = 6.181\n",
      "Epoch   0 Batch 1194/11516   train_loss = 4.078\n",
      "Epoch   0 Batch 1195/11516   train_loss = 5.013\n",
      "Epoch   0 Batch 1196/11516   train_loss = 5.845\n",
      "Epoch   0 Batch 1197/11516   train_loss = 6.405\n",
      "Epoch   0 Batch 1198/11516   train_loss = 7.016\n",
      "Epoch   0 Batch 1199/11516   train_loss = 5.925\n",
      "Epoch   0 Batch 1200/11516   train_loss = 8.247\n",
      "Epoch   0 Batch 1201/11516   train_loss = 6.758\n",
      "Epoch   0 Batch 1202/11516   train_loss = 5.734\n",
      "Epoch   0 Batch 1203/11516   train_loss = 6.474\n",
      "Epoch   0 Batch 1204/11516   train_loss = 7.931\n",
      "Epoch   0 Batch 1205/11516   train_loss = 6.065\n",
      "Epoch   0 Batch 1206/11516   train_loss = 6.266\n",
      "Epoch   0 Batch 1207/11516   train_loss = 8.078\n",
      "Epoch   0 Batch 1208/11516   train_loss = 7.813\n",
      "Epoch   0 Batch 1209/11516   train_loss = 6.719\n",
      "Epoch   0 Batch 1210/11516   train_loss = 10.009\n",
      "Epoch   0 Batch 1211/11516   train_loss = 8.674\n",
      "Epoch   0 Batch 1212/11516   train_loss = 7.654\n",
      "Epoch   0 Batch 1213/11516   train_loss = 9.578\n",
      "Epoch   0 Batch 1214/11516   train_loss = 6.832\n",
      "Epoch   0 Batch 1215/11516   train_loss = 6.946\n",
      "Epoch   0 Batch 1216/11516   train_loss = 7.862\n",
      "Epoch   0 Batch 1217/11516   train_loss = 3.888\n",
      "Epoch   0 Batch 1218/11516   train_loss = 4.387\n",
      "Epoch   0 Batch 1219/11516   train_loss = 7.517\n",
      "Epoch   0 Batch 1220/11516   train_loss = 5.868\n",
      "Epoch   0 Batch 1221/11516   train_loss = 5.317\n",
      "Epoch   0 Batch 1222/11516   train_loss = 5.533\n",
      "Epoch   0 Batch 1223/11516   train_loss = 7.773\n",
      "Epoch   0 Batch 1224/11516   train_loss = 6.233\n",
      "Epoch   0 Batch 1225/11516   train_loss = 6.721\n",
      "Epoch   0 Batch 1226/11516   train_loss = 5.021\n",
      "Epoch   0 Batch 1227/11516   train_loss = 8.805\n",
      "Epoch   0 Batch 1228/11516   train_loss = 7.924\n",
      "Epoch   0 Batch 1229/11516   train_loss = 7.738\n",
      "Epoch   0 Batch 1230/11516   train_loss = 4.195\n",
      "Epoch   0 Batch 1231/11516   train_loss = 5.577\n",
      "Epoch   0 Batch 1232/11516   train_loss = 3.008\n",
      "Epoch   0 Batch 1233/11516   train_loss = 3.663\n",
      "Epoch   0 Batch 1234/11516   train_loss = 5.645\n",
      "Epoch   0 Batch 1235/11516   train_loss = 5.885\n",
      "Epoch   0 Batch 1236/11516   train_loss = 6.004\n",
      "Epoch   0 Batch 1237/11516   train_loss = 4.627\n",
      "Epoch   0 Batch 1238/11516   train_loss = 8.711\n",
      "Epoch   0 Batch 1239/11516   train_loss = 5.618\n",
      "Epoch   0 Batch 1240/11516   train_loss = 4.721\n",
      "Epoch   0 Batch 1241/11516   train_loss = 6.772\n",
      "Epoch   0 Batch 1242/11516   train_loss = 7.654\n",
      "Epoch   0 Batch 1243/11516   train_loss = 7.218\n",
      "Epoch   0 Batch 1244/11516   train_loss = 9.328\n",
      "Epoch   0 Batch 1245/11516   train_loss = 8.565\n",
      "Epoch   0 Batch 1246/11516   train_loss = 4.303\n",
      "Epoch   0 Batch 1247/11516   train_loss = 9.094\n",
      "Epoch   0 Batch 1248/11516   train_loss = 7.250\n",
      "Epoch   0 Batch 1249/11516   train_loss = 3.882\n",
      "Epoch   0 Batch 1250/11516   train_loss = 9.837\n",
      "Epoch   0 Batch 1251/11516   train_loss = 9.275\n",
      "Epoch   0 Batch 1252/11516   train_loss = 6.021\n",
      "Epoch   0 Batch 1253/11516   train_loss = 6.675\n",
      "Epoch   0 Batch 1254/11516   train_loss = 3.656\n",
      "Epoch   0 Batch 1255/11516   train_loss = 3.497\n",
      "Epoch   0 Batch 1256/11516   train_loss = 4.690\n",
      "Epoch   0 Batch 1257/11516   train_loss = 8.764\n",
      "Epoch   0 Batch 1258/11516   train_loss = 6.333\n",
      "Epoch   0 Batch 1259/11516   train_loss = 4.926\n",
      "Epoch   0 Batch 1260/11516   train_loss = 5.490\n",
      "Epoch   0 Batch 1261/11516   train_loss = 7.018\n",
      "Epoch   0 Batch 1262/11516   train_loss = 5.924\n",
      "Epoch   0 Batch 1263/11516   train_loss = 5.705\n",
      "Epoch   0 Batch 1264/11516   train_loss = 4.841\n",
      "Epoch   0 Batch 1265/11516   train_loss = 5.198\n",
      "Epoch   0 Batch 1266/11516   train_loss = 5.809\n",
      "Epoch   0 Batch 1267/11516   train_loss = 5.799\n",
      "Epoch   0 Batch 1268/11516   train_loss = 5.902\n",
      "Epoch   0 Batch 1269/11516   train_loss = 4.699\n",
      "Epoch   0 Batch 1270/11516   train_loss = 6.938\n",
      "Epoch   0 Batch 1271/11516   train_loss = 7.217\n",
      "Epoch   0 Batch 1272/11516   train_loss = 7.414\n",
      "Epoch   0 Batch 1273/11516   train_loss = 9.713\n",
      "Epoch   0 Batch 1274/11516   train_loss = 7.374\n",
      "Epoch   0 Batch 1275/11516   train_loss = 9.129\n",
      "Epoch   0 Batch 1276/11516   train_loss = 6.875\n",
      "Epoch   0 Batch 1277/11516   train_loss = 8.107\n",
      "Epoch   0 Batch 1278/11516   train_loss = 6.104\n",
      "Epoch   0 Batch 1279/11516   train_loss = 8.113\n",
      "Epoch   0 Batch 1280/11516   train_loss = 7.747\n",
      "Epoch   0 Batch 1281/11516   train_loss = 4.831\n",
      "Epoch   0 Batch 1282/11516   train_loss = 6.847\n",
      "Epoch   0 Batch 1283/11516   train_loss = 5.399\n",
      "Epoch   0 Batch 1284/11516   train_loss = 5.744\n",
      "Epoch   0 Batch 1285/11516   train_loss = 6.541\n",
      "Epoch   0 Batch 1286/11516   train_loss = 8.581\n",
      "Epoch   0 Batch 1287/11516   train_loss = 5.863\n",
      "Epoch   0 Batch 1288/11516   train_loss = 6.146\n",
      "Epoch   0 Batch 1289/11516   train_loss = 5.153\n",
      "Epoch   0 Batch 1290/11516   train_loss = 6.852\n",
      "Epoch   0 Batch 1291/11516   train_loss = 6.214\n",
      "Epoch   0 Batch 1292/11516   train_loss = 5.995\n",
      "Epoch   0 Batch 1293/11516   train_loss = 7.461\n",
      "Epoch   0 Batch 1294/11516   train_loss = 5.389\n",
      "Epoch   0 Batch 1295/11516   train_loss = 4.698\n",
      "Epoch   0 Batch 1296/11516   train_loss = 3.373\n",
      "Epoch   0 Batch 1297/11516   train_loss = 6.605\n",
      "Epoch   0 Batch 1298/11516   train_loss = 6.493\n",
      "Epoch   0 Batch 1299/11516   train_loss = 8.349\n",
      "Epoch   0 Batch 1300/11516   train_loss = 7.411\n",
      "Epoch   0 Batch 1301/11516   train_loss = 6.545\n",
      "Epoch   0 Batch 1302/11516   train_loss = 5.933\n",
      "Epoch   0 Batch 1303/11516   train_loss = 7.777\n",
      "Epoch   0 Batch 1304/11516   train_loss = 4.973\n",
      "Epoch   0 Batch 1305/11516   train_loss = 4.530\n",
      "Epoch   0 Batch 1306/11516   train_loss = 5.579\n",
      "Epoch   0 Batch 1307/11516   train_loss = 5.815\n",
      "Epoch   0 Batch 1308/11516   train_loss = 7.313\n",
      "Epoch   0 Batch 1309/11516   train_loss = 3.606\n",
      "Epoch   0 Batch 1310/11516   train_loss = 6.378\n",
      "Epoch   0 Batch 1311/11516   train_loss = 8.417\n",
      "Epoch   0 Batch 1312/11516   train_loss = 7.229\n",
      "Epoch   0 Batch 1313/11516   train_loss = 5.164\n",
      "Epoch   0 Batch 1314/11516   train_loss = 6.028\n",
      "Epoch   0 Batch 1315/11516   train_loss = 8.605\n",
      "Epoch   0 Batch 1316/11516   train_loss = 7.622\n",
      "Epoch   0 Batch 1317/11516   train_loss = 6.624\n",
      "Epoch   0 Batch 1318/11516   train_loss = 3.788\n",
      "Epoch   0 Batch 1319/11516   train_loss = 4.630\n",
      "Epoch   0 Batch 1320/11516   train_loss = 8.760\n",
      "Epoch   0 Batch 1321/11516   train_loss = 6.862\n",
      "Epoch   0 Batch 1322/11516   train_loss = 6.615\n",
      "Epoch   0 Batch 1323/11516   train_loss = 3.298\n",
      "Epoch   0 Batch 1324/11516   train_loss = 4.418\n",
      "Epoch   0 Batch 1325/11516   train_loss = 6.688\n",
      "Epoch   0 Batch 1326/11516   train_loss = 5.813\n",
      "Epoch   0 Batch 1327/11516   train_loss = 7.351\n",
      "Epoch   0 Batch 1328/11516   train_loss = 3.421\n",
      "Epoch   0 Batch 1329/11516   train_loss = 8.110\n",
      "Epoch   0 Batch 1330/11516   train_loss = 7.333\n",
      "Epoch   0 Batch 1331/11516   train_loss = 4.258\n",
      "Epoch   0 Batch 1332/11516   train_loss = 5.228\n",
      "Epoch   0 Batch 1333/11516   train_loss = 7.881\n",
      "Epoch   0 Batch 1334/11516   train_loss = 11.225\n",
      "Epoch   0 Batch 1335/11516   train_loss = 3.467\n",
      "Epoch   0 Batch 1336/11516   train_loss = 5.806\n",
      "Epoch   0 Batch 1337/11516   train_loss = 6.603\n",
      "Epoch   0 Batch 1338/11516   train_loss = 4.780\n",
      "Epoch   0 Batch 1339/11516   train_loss = 6.426\n",
      "Epoch   0 Batch 1340/11516   train_loss = 5.989\n",
      "Epoch   0 Batch 1341/11516   train_loss = 6.530\n",
      "Epoch   0 Batch 1342/11516   train_loss = 10.560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1343/11516   train_loss = 5.041\n",
      "Epoch   0 Batch 1344/11516   train_loss = 9.231\n",
      "Epoch   0 Batch 1345/11516   train_loss = 6.539\n",
      "Epoch   0 Batch 1346/11516   train_loss = 5.403\n",
      "Epoch   0 Batch 1347/11516   train_loss = 8.062\n",
      "Epoch   0 Batch 1348/11516   train_loss = 6.030\n",
      "Epoch   0 Batch 1349/11516   train_loss = 9.138\n",
      "Epoch   0 Batch 1350/11516   train_loss = 3.689\n",
      "Epoch   0 Batch 1351/11516   train_loss = 5.731\n",
      "Epoch   0 Batch 1352/11516   train_loss = 5.335\n",
      "Epoch   0 Batch 1353/11516   train_loss = 4.394\n",
      "Epoch   0 Batch 1354/11516   train_loss = 4.434\n",
      "Epoch   0 Batch 1355/11516   train_loss = 8.916\n",
      "Epoch   0 Batch 1356/11516   train_loss = 4.451\n",
      "Epoch   0 Batch 1357/11516   train_loss = 8.834\n",
      "Epoch   0 Batch 1358/11516   train_loss = 7.012\n",
      "Epoch   0 Batch 1359/11516   train_loss = 6.005\n",
      "Epoch   0 Batch 1360/11516   train_loss = 5.178\n",
      "Epoch   0 Batch 1361/11516   train_loss = 6.688\n",
      "Epoch   0 Batch 1362/11516   train_loss = 9.345\n",
      "Epoch   0 Batch 1363/11516   train_loss = 5.838\n",
      "Epoch   0 Batch 1364/11516   train_loss = 7.326\n",
      "Epoch   0 Batch 1365/11516   train_loss = 6.089\n",
      "Epoch   0 Batch 1366/11516   train_loss = 4.553\n",
      "Epoch   0 Batch 1367/11516   train_loss = 8.658\n",
      "Epoch   0 Batch 1368/11516   train_loss = 6.141\n",
      "Epoch   0 Batch 1369/11516   train_loss = 7.234\n",
      "Epoch   0 Batch 1370/11516   train_loss = 4.971\n",
      "Epoch   0 Batch 1371/11516   train_loss = 5.787\n",
      "Epoch   0 Batch 1372/11516   train_loss = 8.661\n",
      "Epoch   0 Batch 1373/11516   train_loss = 6.183\n",
      "Epoch   0 Batch 1374/11516   train_loss = 6.233\n",
      "Epoch   0 Batch 1375/11516   train_loss = 9.168\n",
      "Epoch   0 Batch 1376/11516   train_loss = 8.331\n",
      "Epoch   0 Batch 1377/11516   train_loss = 9.150\n",
      "Epoch   0 Batch 1378/11516   train_loss = 5.232\n",
      "Epoch   0 Batch 1379/11516   train_loss = 9.111\n",
      "Epoch   0 Batch 1380/11516   train_loss = 4.846\n",
      "Epoch   0 Batch 1381/11516   train_loss = 7.673\n",
      "Epoch   0 Batch 1382/11516   train_loss = 4.776\n",
      "Epoch   0 Batch 1383/11516   train_loss = 6.743\n",
      "Epoch   0 Batch 1384/11516   train_loss = 6.260\n",
      "Epoch   0 Batch 1385/11516   train_loss = 8.098\n",
      "Epoch   0 Batch 1386/11516   train_loss = 3.375\n",
      "Epoch   0 Batch 1387/11516   train_loss = 6.443\n",
      "Epoch   0 Batch 1388/11516   train_loss = 7.087\n",
      "Epoch   0 Batch 1389/11516   train_loss = 6.021\n",
      "Epoch   0 Batch 1390/11516   train_loss = 6.782\n",
      "Epoch   0 Batch 1391/11516   train_loss = 6.902\n",
      "Epoch   0 Batch 1392/11516   train_loss = 6.078\n",
      "Epoch   0 Batch 1393/11516   train_loss = 5.258\n",
      "Epoch   0 Batch 1394/11516   train_loss = 6.552\n",
      "Epoch   0 Batch 1395/11516   train_loss = 4.465\n",
      "Epoch   0 Batch 1396/11516   train_loss = 4.777\n",
      "Epoch   0 Batch 1397/11516   train_loss = 4.512\n",
      "Epoch   0 Batch 1398/11516   train_loss = 3.917\n",
      "Epoch   0 Batch 1399/11516   train_loss = 6.446\n",
      "Epoch   0 Batch 1400/11516   train_loss = 7.742\n",
      "Epoch   0 Batch 1401/11516   train_loss = 6.002\n",
      "Epoch   0 Batch 1402/11516   train_loss = 5.886\n",
      "Epoch   0 Batch 1403/11516   train_loss = 5.594\n",
      "Epoch   0 Batch 1404/11516   train_loss = 5.165\n",
      "Epoch   0 Batch 1405/11516   train_loss = 5.719\n",
      "Epoch   0 Batch 1406/11516   train_loss = 6.031\n",
      "Epoch   0 Batch 1407/11516   train_loss = 5.734\n",
      "Epoch   0 Batch 1408/11516   train_loss = 7.598\n",
      "Epoch   0 Batch 1409/11516   train_loss = 5.878\n",
      "Epoch   0 Batch 1410/11516   train_loss = 4.507\n",
      "Epoch   0 Batch 1411/11516   train_loss = 4.904\n",
      "Epoch   0 Batch 1412/11516   train_loss = 7.678\n",
      "Epoch   0 Batch 1413/11516   train_loss = 4.634\n",
      "Epoch   0 Batch 1414/11516   train_loss = 3.874\n",
      "Epoch   0 Batch 1415/11516   train_loss = 3.896\n",
      "Epoch   0 Batch 1416/11516   train_loss = 5.067\n",
      "Epoch   0 Batch 1417/11516   train_loss = 5.286\n",
      "Epoch   0 Batch 1418/11516   train_loss = 8.283\n",
      "Epoch   0 Batch 1419/11516   train_loss = 6.818\n",
      "Epoch   0 Batch 1420/11516   train_loss = 5.334\n",
      "Epoch   0 Batch 1421/11516   train_loss = 4.309\n",
      "Epoch   0 Batch 1422/11516   train_loss = 5.063\n",
      "Epoch   0 Batch 1423/11516   train_loss = 3.674\n",
      "Epoch   0 Batch 1424/11516   train_loss = 5.166\n",
      "Epoch   0 Batch 1425/11516   train_loss = 9.163\n",
      "Epoch   0 Batch 1426/11516   train_loss = 4.249\n",
      "Epoch   0 Batch 1427/11516   train_loss = 4.870\n",
      "Epoch   0 Batch 1428/11516   train_loss = 4.421\n",
      "Epoch   0 Batch 1429/11516   train_loss = 5.992\n",
      "Epoch   0 Batch 1430/11516   train_loss = 8.190\n",
      "Epoch   0 Batch 1431/11516   train_loss = 3.493\n",
      "Epoch   0 Batch 1432/11516   train_loss = 7.942\n",
      "Epoch   0 Batch 1433/11516   train_loss = 8.520\n",
      "Epoch   0 Batch 1434/11516   train_loss = 6.165\n",
      "Epoch   0 Batch 1435/11516   train_loss = 8.167\n",
      "Epoch   0 Batch 1436/11516   train_loss = 4.454\n",
      "Epoch   0 Batch 1437/11516   train_loss = 6.157\n",
      "Epoch   0 Batch 1438/11516   train_loss = 5.165\n",
      "Epoch   0 Batch 1439/11516   train_loss = 7.746\n",
      "Epoch   0 Batch 1440/11516   train_loss = 5.882\n",
      "Epoch   0 Batch 1441/11516   train_loss = 6.998\n",
      "Epoch   0 Batch 1442/11516   train_loss = 4.916\n",
      "Epoch   0 Batch 1443/11516   train_loss = 7.689\n",
      "Epoch   0 Batch 1444/11516   train_loss = 6.390\n",
      "Epoch   0 Batch 1445/11516   train_loss = 6.548\n",
      "Epoch   0 Batch 1446/11516   train_loss = 3.350\n",
      "Epoch   0 Batch 1447/11516   train_loss = 6.286\n",
      "Epoch   0 Batch 1448/11516   train_loss = 4.233\n",
      "Epoch   0 Batch 1449/11516   train_loss = 7.002\n",
      "Epoch   0 Batch 1450/11516   train_loss = 3.667\n",
      "Epoch   0 Batch 1451/11516   train_loss = 5.339\n",
      "Epoch   0 Batch 1452/11516   train_loss = 7.656\n",
      "Epoch   0 Batch 1453/11516   train_loss = 4.778\n",
      "Epoch   0 Batch 1454/11516   train_loss = 6.999\n",
      "Epoch   0 Batch 1455/11516   train_loss = 8.036\n",
      "Epoch   0 Batch 1456/11516   train_loss = 4.790\n",
      "Epoch   0 Batch 1457/11516   train_loss = 7.572\n",
      "Epoch   0 Batch 1458/11516   train_loss = 4.589\n",
      "Epoch   0 Batch 1459/11516   train_loss = 3.908\n",
      "Epoch   0 Batch 1460/11516   train_loss = 4.632\n",
      "Epoch   0 Batch 1461/11516   train_loss = 7.600\n",
      "Epoch   0 Batch 1462/11516   train_loss = 7.777\n",
      "Epoch   0 Batch 1463/11516   train_loss = 9.306\n",
      "Epoch   0 Batch 1464/11516   train_loss = 6.713\n",
      "Epoch   0 Batch 1465/11516   train_loss = 7.750\n",
      "Epoch   0 Batch 1466/11516   train_loss = 4.820\n",
      "Epoch   0 Batch 1467/11516   train_loss = 8.985\n",
      "Epoch   0 Batch 1468/11516   train_loss = 9.692\n",
      "Epoch   0 Batch 1469/11516   train_loss = 5.682\n",
      "Epoch   0 Batch 1470/11516   train_loss = 6.009\n",
      "Epoch   0 Batch 1471/11516   train_loss = 7.415\n",
      "Epoch   0 Batch 1472/11516   train_loss = 8.674\n",
      "Epoch   0 Batch 1473/11516   train_loss = 3.378\n",
      "Epoch   0 Batch 1474/11516   train_loss = 8.738\n",
      "Epoch   0 Batch 1475/11516   train_loss = 7.151\n",
      "Epoch   0 Batch 1476/11516   train_loss = 6.614\n",
      "Epoch   0 Batch 1477/11516   train_loss = 6.614\n",
      "Epoch   0 Batch 1478/11516   train_loss = 3.565\n",
      "Epoch   0 Batch 1479/11516   train_loss = 5.865\n",
      "Epoch   0 Batch 1480/11516   train_loss = 4.569\n",
      "Epoch   0 Batch 1481/11516   train_loss = 8.005\n",
      "Epoch   0 Batch 1482/11516   train_loss = 9.445\n",
      "Epoch   0 Batch 1483/11516   train_loss = 3.979\n",
      "Epoch   0 Batch 1484/11516   train_loss = 7.052\n",
      "Epoch   0 Batch 1485/11516   train_loss = 4.925\n",
      "Epoch   0 Batch 1486/11516   train_loss = 4.200\n",
      "Epoch   0 Batch 1487/11516   train_loss = 5.745\n",
      "Epoch   0 Batch 1488/11516   train_loss = 7.856\n",
      "Epoch   0 Batch 1489/11516   train_loss = 4.510\n",
      "Epoch   0 Batch 1490/11516   train_loss = 6.753\n",
      "Epoch   0 Batch 1491/11516   train_loss = 5.213\n",
      "Epoch   0 Batch 1492/11516   train_loss = 3.311\n",
      "Epoch   0 Batch 1493/11516   train_loss = 7.324\n",
      "Epoch   0 Batch 1494/11516   train_loss = 6.974\n",
      "Epoch   0 Batch 1495/11516   train_loss = 5.374\n",
      "Epoch   0 Batch 1496/11516   train_loss = 7.263\n",
      "Epoch   0 Batch 1497/11516   train_loss = 5.877\n",
      "Epoch   0 Batch 1498/11516   train_loss = 6.995\n",
      "Epoch   0 Batch 1499/11516   train_loss = 6.799\n",
      "Epoch   0 Batch 1500/11516   train_loss = 6.588\n",
      "Epoch   0 Batch 1501/11516   train_loss = 6.502\n",
      "Epoch   0 Batch 1502/11516   train_loss = 4.203\n",
      "Epoch   0 Batch 1503/11516   train_loss = 6.815\n",
      "Epoch   0 Batch 1504/11516   train_loss = 7.614\n",
      "Epoch   0 Batch 1505/11516   train_loss = 7.565\n",
      "Epoch   0 Batch 1506/11516   train_loss = 7.046\n",
      "Epoch   0 Batch 1507/11516   train_loss = 6.862\n",
      "Epoch   0 Batch 1508/11516   train_loss = 4.227\n",
      "Epoch   0 Batch 1509/11516   train_loss = 4.539\n",
      "Epoch   0 Batch 1510/11516   train_loss = 5.893\n",
      "Epoch   0 Batch 1511/11516   train_loss = 7.062\n",
      "Epoch   0 Batch 1512/11516   train_loss = 8.438\n",
      "Epoch   0 Batch 1513/11516   train_loss = 6.038\n",
      "Epoch   0 Batch 1514/11516   train_loss = 5.694\n",
      "Epoch   0 Batch 1515/11516   train_loss = 6.748\n",
      "Epoch   0 Batch 1516/11516   train_loss = 5.674\n",
      "Epoch   0 Batch 1517/11516   train_loss = 6.650\n",
      "Epoch   0 Batch 1518/11516   train_loss = 3.579\n",
      "Epoch   0 Batch 1519/11516   train_loss = 7.638\n",
      "Epoch   0 Batch 1520/11516   train_loss = 4.127\n",
      "Epoch   0 Batch 1521/11516   train_loss = 5.633\n",
      "Epoch   0 Batch 1522/11516   train_loss = 7.827\n",
      "Epoch   0 Batch 1523/11516   train_loss = 5.642\n",
      "Epoch   0 Batch 1524/11516   train_loss = 4.223\n",
      "Epoch   0 Batch 1525/11516   train_loss = 7.022\n",
      "Epoch   0 Batch 1526/11516   train_loss = 4.647\n",
      "Epoch   0 Batch 1527/11516   train_loss = 4.775\n",
      "Epoch   0 Batch 1528/11516   train_loss = 7.352\n",
      "Epoch   0 Batch 1529/11516   train_loss = 7.040\n",
      "Epoch   0 Batch 1530/11516   train_loss = 12.842\n",
      "Epoch   0 Batch 1531/11516   train_loss = 4.901\n",
      "Epoch   0 Batch 1532/11516   train_loss = 4.471\n",
      "Epoch   0 Batch 1533/11516   train_loss = 2.891\n",
      "Epoch   0 Batch 1534/11516   train_loss = 5.497\n",
      "Epoch   0 Batch 1535/11516   train_loss = 8.173\n",
      "Epoch   0 Batch 1536/11516   train_loss = 6.046\n",
      "Epoch   0 Batch 1537/11516   train_loss = 5.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1538/11516   train_loss = 3.977\n",
      "Epoch   0 Batch 1539/11516   train_loss = 5.816\n",
      "Epoch   0 Batch 1540/11516   train_loss = 5.967\n",
      "Epoch   0 Batch 1541/11516   train_loss = 5.341\n",
      "Epoch   0 Batch 1542/11516   train_loss = 7.061\n",
      "Epoch   0 Batch 1543/11516   train_loss = 7.422\n",
      "Epoch   0 Batch 1544/11516   train_loss = 5.267\n",
      "Epoch   0 Batch 1545/11516   train_loss = 3.904\n",
      "Epoch   0 Batch 1546/11516   train_loss = 4.029\n",
      "Epoch   0 Batch 1547/11516   train_loss = 5.738\n",
      "Epoch   0 Batch 1548/11516   train_loss = 7.790\n",
      "Epoch   0 Batch 1549/11516   train_loss = 8.086\n",
      "Epoch   0 Batch 1550/11516   train_loss = 4.969\n",
      "Epoch   0 Batch 1551/11516   train_loss = 3.958\n",
      "Epoch   0 Batch 1552/11516   train_loss = 6.108\n",
      "Epoch   0 Batch 1553/11516   train_loss = 5.107\n",
      "Epoch   0 Batch 1554/11516   train_loss = 9.574\n",
      "Epoch   0 Batch 1555/11516   train_loss = 7.456\n",
      "Epoch   0 Batch 1556/11516   train_loss = 6.313\n",
      "Epoch   0 Batch 1557/11516   train_loss = 9.852\n",
      "Epoch   0 Batch 1558/11516   train_loss = 11.586\n",
      "Epoch   0 Batch 1559/11516   train_loss = 5.424\n",
      "Epoch   0 Batch 1560/11516   train_loss = 5.995\n",
      "Epoch   0 Batch 1561/11516   train_loss = 6.986\n",
      "Epoch   0 Batch 1562/11516   train_loss = 9.571\n",
      "Epoch   0 Batch 1563/11516   train_loss = 7.117\n",
      "Epoch   0 Batch 1564/11516   train_loss = 8.915\n",
      "Epoch   0 Batch 1565/11516   train_loss = 4.653\n",
      "Epoch   0 Batch 1566/11516   train_loss = 6.926\n",
      "Epoch   0 Batch 1567/11516   train_loss = 7.020\n",
      "Epoch   0 Batch 1568/11516   train_loss = 4.743\n",
      "Epoch   0 Batch 1569/11516   train_loss = 4.974\n",
      "Epoch   0 Batch 1570/11516   train_loss = 8.875\n",
      "Epoch   0 Batch 1571/11516   train_loss = 5.730\n",
      "Epoch   0 Batch 1572/11516   train_loss = 5.556\n",
      "Epoch   0 Batch 1573/11516   train_loss = 5.305\n",
      "Epoch   0 Batch 1574/11516   train_loss = 7.335\n",
      "Epoch   0 Batch 1575/11516   train_loss = 6.201\n",
      "Epoch   0 Batch 1576/11516   train_loss = 6.095\n",
      "Epoch   0 Batch 1577/11516   train_loss = 5.998\n",
      "Epoch   0 Batch 1578/11516   train_loss = 6.684\n",
      "Epoch   0 Batch 1579/11516   train_loss = 5.821\n",
      "Epoch   0 Batch 1580/11516   train_loss = 5.909\n",
      "Epoch   0 Batch 1581/11516   train_loss = 9.402\n",
      "Epoch   0 Batch 1582/11516   train_loss = 6.469\n",
      "Epoch   0 Batch 1583/11516   train_loss = 8.724\n",
      "Epoch   0 Batch 1584/11516   train_loss = 7.960\n",
      "Epoch   0 Batch 1585/11516   train_loss = 7.629\n",
      "Epoch   0 Batch 1586/11516   train_loss = 7.508\n",
      "Epoch   0 Batch 1587/11516   train_loss = 4.386\n",
      "Epoch   0 Batch 1588/11516   train_loss = 5.286\n",
      "Epoch   0 Batch 1589/11516   train_loss = 5.831\n",
      "Epoch   0 Batch 1590/11516   train_loss = 6.014\n",
      "Epoch   0 Batch 1591/11516   train_loss = 7.279\n",
      "Epoch   0 Batch 1592/11516   train_loss = 7.482\n",
      "Epoch   0 Batch 1593/11516   train_loss = 8.438\n",
      "Epoch   0 Batch 1594/11516   train_loss = 2.540\n",
      "Epoch   0 Batch 1595/11516   train_loss = 4.333\n",
      "Epoch   0 Batch 1596/11516   train_loss = 6.197\n",
      "Epoch   0 Batch 1597/11516   train_loss = 6.794\n",
      "Epoch   0 Batch 1598/11516   train_loss = 7.843\n",
      "Epoch   0 Batch 1599/11516   train_loss = 8.972\n",
      "Epoch   0 Batch 1600/11516   train_loss = 5.198\n",
      "Epoch   0 Batch 1601/11516   train_loss = 5.250\n",
      "Epoch   0 Batch 1602/11516   train_loss = 8.003\n",
      "Epoch   0 Batch 1603/11516   train_loss = 6.901\n",
      "Epoch   0 Batch 1604/11516   train_loss = 3.585\n",
      "Epoch   0 Batch 1605/11516   train_loss = 5.697\n",
      "Epoch   0 Batch 1606/11516   train_loss = 8.625\n",
      "Epoch   0 Batch 1607/11516   train_loss = 4.283\n",
      "Epoch   0 Batch 1608/11516   train_loss = 4.138\n",
      "Epoch   0 Batch 1609/11516   train_loss = 4.103\n",
      "Epoch   0 Batch 1610/11516   train_loss = 5.141\n",
      "Epoch   0 Batch 1611/11516   train_loss = 7.491\n",
      "Epoch   0 Batch 1612/11516   train_loss = 5.051\n",
      "Epoch   0 Batch 1613/11516   train_loss = 9.159\n",
      "Epoch   0 Batch 1614/11516   train_loss = 6.535\n",
      "Epoch   0 Batch 1615/11516   train_loss = 6.647\n",
      "Epoch   0 Batch 1616/11516   train_loss = 5.351\n",
      "Epoch   0 Batch 1617/11516   train_loss = 5.598\n",
      "Epoch   0 Batch 1618/11516   train_loss = 5.662\n",
      "Epoch   0 Batch 1619/11516   train_loss = 5.437\n",
      "Epoch   0 Batch 1620/11516   train_loss = 5.948\n",
      "Epoch   0 Batch 1621/11516   train_loss = 3.894\n",
      "Epoch   0 Batch 1622/11516   train_loss = 5.134\n",
      "Epoch   0 Batch 1623/11516   train_loss = 9.962\n",
      "Epoch   0 Batch 1624/11516   train_loss = 3.980\n",
      "Epoch   0 Batch 1625/11516   train_loss = 6.120\n",
      "Epoch   0 Batch 1626/11516   train_loss = 3.866\n",
      "Epoch   0 Batch 1627/11516   train_loss = 10.200\n",
      "Epoch   0 Batch 1628/11516   train_loss = 10.356\n",
      "Epoch   0 Batch 1629/11516   train_loss = 9.016\n",
      "Epoch   0 Batch 1630/11516   train_loss = 4.181\n",
      "Epoch   0 Batch 1631/11516   train_loss = 5.764\n",
      "Epoch   0 Batch 1632/11516   train_loss = 4.916\n",
      "Epoch   0 Batch 1633/11516   train_loss = 5.455\n",
      "Epoch   0 Batch 1634/11516   train_loss = 9.779\n",
      "Epoch   0 Batch 1635/11516   train_loss = 5.961\n",
      "Epoch   0 Batch 1636/11516   train_loss = 5.699\n",
      "Epoch   0 Batch 1637/11516   train_loss = 7.005\n",
      "Epoch   0 Batch 1638/11516   train_loss = 7.450\n",
      "Epoch   0 Batch 1639/11516   train_loss = 5.038\n",
      "Epoch   0 Batch 1640/11516   train_loss = 3.130\n",
      "Epoch   0 Batch 1641/11516   train_loss = 7.094\n",
      "Epoch   0 Batch 1642/11516   train_loss = 6.213\n",
      "Epoch   0 Batch 1643/11516   train_loss = 4.951\n",
      "Epoch   0 Batch 1644/11516   train_loss = 8.027\n",
      "Epoch   0 Batch 1645/11516   train_loss = 9.703\n",
      "Epoch   0 Batch 1646/11516   train_loss = 4.419\n",
      "Epoch   0 Batch 1647/11516   train_loss = 7.245\n",
      "Epoch   0 Batch 1648/11516   train_loss = 5.592\n",
      "Epoch   0 Batch 1649/11516   train_loss = 7.710\n",
      "Epoch   0 Batch 1650/11516   train_loss = 6.972\n",
      "Epoch   0 Batch 1651/11516   train_loss = 5.311\n",
      "Epoch   0 Batch 1652/11516   train_loss = 6.269\n",
      "Epoch   0 Batch 1653/11516   train_loss = 7.156\n",
      "Epoch   0 Batch 1654/11516   train_loss = 5.083\n",
      "Epoch   0 Batch 1655/11516   train_loss = 7.815\n",
      "Epoch   0 Batch 1656/11516   train_loss = 5.500\n",
      "Epoch   0 Batch 1657/11516   train_loss = 6.719\n",
      "Epoch   0 Batch 1658/11516   train_loss = 5.914\n",
      "Epoch   0 Batch 1659/11516   train_loss = 4.007\n",
      "Epoch   0 Batch 1660/11516   train_loss = 6.764\n",
      "Epoch   0 Batch 1661/11516   train_loss = 11.123\n",
      "Epoch   0 Batch 1662/11516   train_loss = 10.287\n",
      "Epoch   0 Batch 1663/11516   train_loss = 9.781\n",
      "Epoch   0 Batch 1664/11516   train_loss = 10.626\n",
      "Epoch   0 Batch 1665/11516   train_loss = 7.229\n",
      "Epoch   0 Batch 1666/11516   train_loss = 4.068\n",
      "Epoch   0 Batch 1667/11516   train_loss = 4.817\n",
      "Epoch   0 Batch 1668/11516   train_loss = 5.158\n",
      "Epoch   0 Batch 1669/11516   train_loss = 5.553\n",
      "Epoch   0 Batch 1670/11516   train_loss = 8.303\n",
      "Epoch   0 Batch 1671/11516   train_loss = 9.826\n",
      "Epoch   0 Batch 1672/11516   train_loss = 9.066\n",
      "Epoch   0 Batch 1673/11516   train_loss = 4.403\n",
      "Epoch   0 Batch 1674/11516   train_loss = 6.785\n",
      "Epoch   0 Batch 1675/11516   train_loss = 7.164\n",
      "Epoch   0 Batch 1676/11516   train_loss = 8.139\n",
      "Epoch   0 Batch 1677/11516   train_loss = 5.199\n",
      "Epoch   0 Batch 1678/11516   train_loss = 5.537\n",
      "Epoch   0 Batch 1679/11516   train_loss = 4.500\n",
      "Epoch   0 Batch 1680/11516   train_loss = 6.803\n",
      "Epoch   0 Batch 1681/11516   train_loss = 5.534\n",
      "Epoch   0 Batch 1682/11516   train_loss = 5.500\n",
      "Epoch   0 Batch 1683/11516   train_loss = 8.308\n",
      "Epoch   0 Batch 1684/11516   train_loss = 5.005\n",
      "Epoch   0 Batch 1685/11516   train_loss = 7.115\n",
      "Epoch   0 Batch 1686/11516   train_loss = 5.205\n",
      "Epoch   0 Batch 1687/11516   train_loss = 7.269\n",
      "Epoch   0 Batch 1688/11516   train_loss = 6.054\n",
      "Epoch   0 Batch 1689/11516   train_loss = 7.265\n",
      "Epoch   0 Batch 1690/11516   train_loss = 5.393\n",
      "Epoch   0 Batch 1691/11516   train_loss = 4.905\n",
      "Epoch   0 Batch 1692/11516   train_loss = 4.162\n",
      "Epoch   0 Batch 1693/11516   train_loss = 7.845\n",
      "Epoch   0 Batch 1694/11516   train_loss = 7.198\n",
      "Epoch   0 Batch 1695/11516   train_loss = 3.125\n",
      "Epoch   0 Batch 1696/11516   train_loss = 7.604\n",
      "Epoch   0 Batch 1697/11516   train_loss = 4.386\n",
      "Epoch   0 Batch 1698/11516   train_loss = 5.071\n",
      "Epoch   0 Batch 1699/11516   train_loss = 7.120\n",
      "Epoch   0 Batch 1700/11516   train_loss = 8.310\n",
      "Epoch   0 Batch 1701/11516   train_loss = 4.669\n",
      "Epoch   0 Batch 1702/11516   train_loss = 9.264\n",
      "Epoch   0 Batch 1703/11516   train_loss = 7.997\n",
      "Epoch   0 Batch 1704/11516   train_loss = 5.556\n",
      "Epoch   0 Batch 1705/11516   train_loss = 4.104\n",
      "Epoch   0 Batch 1706/11516   train_loss = 7.673\n",
      "Epoch   0 Batch 1707/11516   train_loss = 6.115\n",
      "Epoch   0 Batch 1708/11516   train_loss = 5.497\n",
      "Epoch   0 Batch 1709/11516   train_loss = 4.614\n",
      "Epoch   0 Batch 1710/11516   train_loss = 7.560\n",
      "Epoch   0 Batch 1711/11516   train_loss = 9.470\n",
      "Epoch   0 Batch 1712/11516   train_loss = 3.977\n",
      "Epoch   0 Batch 1713/11516   train_loss = 8.671\n",
      "Epoch   0 Batch 1714/11516   train_loss = 4.696\n",
      "Epoch   0 Batch 1715/11516   train_loss = 6.664\n",
      "Epoch   0 Batch 1716/11516   train_loss = 6.686\n",
      "Epoch   0 Batch 1717/11516   train_loss = 7.159\n",
      "Epoch   0 Batch 1718/11516   train_loss = 6.021\n",
      "Epoch   0 Batch 1719/11516   train_loss = 5.640\n",
      "Epoch   0 Batch 1720/11516   train_loss = 8.025\n",
      "Epoch   0 Batch 1721/11516   train_loss = 2.777\n",
      "Epoch   0 Batch 1722/11516   train_loss = 3.047\n",
      "Epoch   0 Batch 1723/11516   train_loss = 6.411\n",
      "Epoch   0 Batch 1724/11516   train_loss = 4.958\n",
      "Epoch   0 Batch 1725/11516   train_loss = 4.330\n",
      "Epoch   0 Batch 1726/11516   train_loss = 3.165\n",
      "Epoch   0 Batch 1727/11516   train_loss = 3.181\n",
      "Epoch   0 Batch 1728/11516   train_loss = 5.708\n",
      "Epoch   0 Batch 1729/11516   train_loss = 8.323\n",
      "Epoch   0 Batch 1730/11516   train_loss = 7.095\n",
      "Epoch   0 Batch 1731/11516   train_loss = 5.273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1732/11516   train_loss = 7.226\n",
      "Epoch   0 Batch 1733/11516   train_loss = 4.914\n",
      "Epoch   0 Batch 1734/11516   train_loss = 8.764\n",
      "Epoch   0 Batch 1735/11516   train_loss = 8.922\n",
      "Epoch   0 Batch 1736/11516   train_loss = 6.172\n",
      "Epoch   0 Batch 1737/11516   train_loss = 6.547\n",
      "Epoch   0 Batch 1738/11516   train_loss = 6.086\n",
      "Epoch   0 Batch 1739/11516   train_loss = 4.290\n",
      "Epoch   0 Batch 1740/11516   train_loss = 5.721\n",
      "Epoch   0 Batch 1741/11516   train_loss = 3.860\n",
      "Epoch   0 Batch 1742/11516   train_loss = 8.433\n",
      "Epoch   0 Batch 1743/11516   train_loss = 6.897\n",
      "Epoch   0 Batch 1744/11516   train_loss = 4.457\n",
      "Epoch   0 Batch 1745/11516   train_loss = 9.861\n",
      "Epoch   0 Batch 1746/11516   train_loss = 8.178\n",
      "Epoch   0 Batch 1747/11516   train_loss = 3.605\n",
      "Epoch   0 Batch 1748/11516   train_loss = 5.882\n",
      "Epoch   0 Batch 1749/11516   train_loss = 7.799\n",
      "Epoch   0 Batch 1750/11516   train_loss = 7.577\n",
      "Epoch   0 Batch 1751/11516   train_loss = 10.025\n",
      "Epoch   0 Batch 1752/11516   train_loss = 7.640\n",
      "Epoch   0 Batch 1753/11516   train_loss = 4.840\n",
      "Epoch   0 Batch 1754/11516   train_loss = 6.993\n",
      "Epoch   0 Batch 1755/11516   train_loss = 7.398\n",
      "Epoch   0 Batch 1756/11516   train_loss = 5.934\n",
      "Epoch   0 Batch 1757/11516   train_loss = 8.251\n",
      "Epoch   0 Batch 1758/11516   train_loss = 7.131\n",
      "Epoch   0 Batch 1759/11516   train_loss = 8.875\n",
      "Epoch   0 Batch 1760/11516   train_loss = 3.683\n",
      "Epoch   0 Batch 1761/11516   train_loss = 5.143\n",
      "Epoch   0 Batch 1762/11516   train_loss = 7.480\n",
      "Epoch   0 Batch 1763/11516   train_loss = 10.939\n",
      "Epoch   0 Batch 1764/11516   train_loss = 7.597\n",
      "Epoch   0 Batch 1765/11516   train_loss = 7.553\n",
      "Epoch   0 Batch 1766/11516   train_loss = 7.411\n",
      "Epoch   0 Batch 1767/11516   train_loss = 9.428\n",
      "Epoch   0 Batch 1768/11516   train_loss = 7.353\n",
      "Epoch   0 Batch 1769/11516   train_loss = 3.002\n",
      "Epoch   0 Batch 1770/11516   train_loss = 6.101\n",
      "Epoch   0 Batch 1771/11516   train_loss = 7.924\n",
      "Epoch   0 Batch 1772/11516   train_loss = 8.615\n",
      "Epoch   0 Batch 1773/11516   train_loss = 5.961\n",
      "Epoch   0 Batch 1774/11516   train_loss = 9.227\n",
      "Epoch   0 Batch 1775/11516   train_loss = 3.533\n",
      "Epoch   0 Batch 1776/11516   train_loss = 8.550\n",
      "Epoch   0 Batch 1777/11516   train_loss = 7.931\n",
      "Epoch   0 Batch 1778/11516   train_loss = 7.402\n",
      "Epoch   0 Batch 1779/11516   train_loss = 8.509\n",
      "Epoch   0 Batch 1780/11516   train_loss = 7.129\n",
      "Epoch   0 Batch 1781/11516   train_loss = 6.974\n",
      "Epoch   0 Batch 1782/11516   train_loss = 7.229\n",
      "Epoch   0 Batch 1783/11516   train_loss = 4.464\n",
      "Epoch   0 Batch 1784/11516   train_loss = 5.620\n",
      "Epoch   0 Batch 1785/11516   train_loss = 5.956\n",
      "Epoch   0 Batch 1786/11516   train_loss = 7.980\n",
      "Epoch   0 Batch 1787/11516   train_loss = 8.564\n",
      "Epoch   0 Batch 1788/11516   train_loss = 6.933\n",
      "Epoch   0 Batch 1789/11516   train_loss = 7.623\n",
      "Epoch   0 Batch 1790/11516   train_loss = 7.656\n",
      "Epoch   0 Batch 1791/11516   train_loss = 5.423\n",
      "Epoch   0 Batch 1792/11516   train_loss = 7.102\n",
      "Epoch   0 Batch 1793/11516   train_loss = 8.187\n",
      "Epoch   0 Batch 1794/11516   train_loss = 5.014\n",
      "Epoch   0 Batch 1795/11516   train_loss = 4.317\n",
      "Epoch   0 Batch 1796/11516   train_loss = 6.189\n",
      "Epoch   0 Batch 1797/11516   train_loss = 8.133\n",
      "Epoch   0 Batch 1798/11516   train_loss = 5.075\n",
      "Epoch   0 Batch 1799/11516   train_loss = 5.905\n",
      "Epoch   0 Batch 1800/11516   train_loss = 7.468\n",
      "Epoch   0 Batch 1801/11516   train_loss = 7.603\n",
      "Epoch   0 Batch 1802/11516   train_loss = 7.000\n",
      "Epoch   0 Batch 1803/11516   train_loss = 5.710\n",
      "Epoch   0 Batch 1804/11516   train_loss = 5.822\n",
      "Epoch   0 Batch 1805/11516   train_loss = 7.444\n",
      "Epoch   0 Batch 1806/11516   train_loss = 4.082\n",
      "Epoch   0 Batch 1807/11516   train_loss = 5.823\n",
      "Epoch   0 Batch 1808/11516   train_loss = 5.937\n",
      "Epoch   0 Batch 1809/11516   train_loss = 6.259\n",
      "Epoch   0 Batch 1810/11516   train_loss = 6.969\n",
      "Epoch   0 Batch 1811/11516   train_loss = 6.613\n",
      "Epoch   0 Batch 1812/11516   train_loss = 11.398\n",
      "Epoch   0 Batch 1813/11516   train_loss = 5.339\n",
      "Epoch   0 Batch 1814/11516   train_loss = 8.395\n",
      "Epoch   0 Batch 1815/11516   train_loss = 9.848\n",
      "Epoch   0 Batch 1816/11516   train_loss = 5.694\n",
      "Epoch   0 Batch 1817/11516   train_loss = 4.604\n",
      "Epoch   0 Batch 1818/11516   train_loss = 5.258\n",
      "Epoch   0 Batch 1819/11516   train_loss = 7.527\n",
      "Epoch   0 Batch 1820/11516   train_loss = 8.659\n",
      "Epoch   0 Batch 1821/11516   train_loss = 6.275\n",
      "Epoch   0 Batch 1822/11516   train_loss = 4.738\n",
      "Epoch   0 Batch 1823/11516   train_loss = 7.611\n",
      "Epoch   0 Batch 1824/11516   train_loss = 7.111\n",
      "Epoch   0 Batch 1825/11516   train_loss = 8.107\n",
      "Epoch   0 Batch 1826/11516   train_loss = 7.708\n",
      "Epoch   0 Batch 1827/11516   train_loss = 8.995\n",
      "Epoch   0 Batch 1828/11516   train_loss = 9.133\n",
      "Epoch   0 Batch 1829/11516   train_loss = 5.528\n",
      "Epoch   0 Batch 1830/11516   train_loss = 4.703\n",
      "Epoch   0 Batch 1831/11516   train_loss = 7.626\n",
      "Epoch   0 Batch 1832/11516   train_loss = 6.816\n",
      "Epoch   0 Batch 1833/11516   train_loss = 5.900\n",
      "Epoch   0 Batch 1834/11516   train_loss = 8.746\n",
      "Epoch   0 Batch 1835/11516   train_loss = 7.602\n",
      "Epoch   0 Batch 1836/11516   train_loss = 6.008\n",
      "Epoch   0 Batch 1837/11516   train_loss = 7.482\n",
      "Epoch   0 Batch 1838/11516   train_loss = 6.674\n",
      "Epoch   0 Batch 1839/11516   train_loss = 5.492\n",
      "Epoch   0 Batch 1840/11516   train_loss = 9.141\n",
      "Epoch   0 Batch 1841/11516   train_loss = 8.244\n",
      "Epoch   0 Batch 1842/11516   train_loss = 7.716\n",
      "Epoch   0 Batch 1843/11516   train_loss = 6.770\n",
      "Epoch   0 Batch 1844/11516   train_loss = 6.487\n",
      "Epoch   0 Batch 1845/11516   train_loss = 10.658\n",
      "Epoch   0 Batch 1846/11516   train_loss = 6.358\n",
      "Epoch   0 Batch 1847/11516   train_loss = 7.683\n",
      "Epoch   0 Batch 1848/11516   train_loss = 6.256\n",
      "Epoch   0 Batch 1849/11516   train_loss = 5.779\n",
      "Epoch   0 Batch 1850/11516   train_loss = 4.939\n",
      "Epoch   0 Batch 1851/11516   train_loss = 6.203\n",
      "Epoch   0 Batch 1852/11516   train_loss = 6.157\n",
      "Epoch   0 Batch 1853/11516   train_loss = 10.037\n",
      "Epoch   0 Batch 1854/11516   train_loss = 6.924\n",
      "Epoch   0 Batch 1855/11516   train_loss = 8.489\n",
      "Epoch   0 Batch 1856/11516   train_loss = 4.803\n",
      "Epoch   0 Batch 1857/11516   train_loss = 4.618\n",
      "Epoch   0 Batch 1858/11516   train_loss = 8.204\n",
      "Epoch   0 Batch 1859/11516   train_loss = 6.173\n",
      "Epoch   0 Batch 1860/11516   train_loss = 9.014\n",
      "Epoch   0 Batch 1861/11516   train_loss = 8.414\n",
      "Epoch   0 Batch 1862/11516   train_loss = 9.006\n",
      "Epoch   0 Batch 1863/11516   train_loss = 5.262\n",
      "Epoch   0 Batch 1864/11516   train_loss = 10.472\n",
      "Epoch   0 Batch 1865/11516   train_loss = 4.306\n",
      "Epoch   0 Batch 1866/11516   train_loss = 7.982\n",
      "Epoch   0 Batch 1867/11516   train_loss = 7.788\n",
      "Epoch   0 Batch 1868/11516   train_loss = 5.094\n",
      "Epoch   0 Batch 1869/11516   train_loss = 9.137\n",
      "Epoch   0 Batch 1870/11516   train_loss = 6.283\n",
      "Epoch   0 Batch 1871/11516   train_loss = 6.973\n",
      "Epoch   0 Batch 1872/11516   train_loss = 7.758\n",
      "Epoch   0 Batch 1873/11516   train_loss = 10.160\n",
      "Epoch   0 Batch 1874/11516   train_loss = 6.725\n",
      "Epoch   0 Batch 1875/11516   train_loss = 9.073\n",
      "Epoch   0 Batch 1876/11516   train_loss = 5.309\n",
      "Epoch   0 Batch 1877/11516   train_loss = 7.924\n",
      "Epoch   0 Batch 1878/11516   train_loss = 10.535\n",
      "Epoch   0 Batch 1879/11516   train_loss = 7.103\n",
      "Epoch   0 Batch 1880/11516   train_loss = 7.828\n",
      "Epoch   0 Batch 1881/11516   train_loss = 5.125\n",
      "Epoch   0 Batch 1882/11516   train_loss = 6.238\n",
      "Epoch   0 Batch 1883/11516   train_loss = 7.634\n",
      "Epoch   0 Batch 1884/11516   train_loss = 8.467\n",
      "Epoch   0 Batch 1885/11516   train_loss = 7.560\n",
      "Epoch   0 Batch 1886/11516   train_loss = 9.460\n",
      "Epoch   0 Batch 1887/11516   train_loss = 7.151\n",
      "Epoch   0 Batch 1888/11516   train_loss = 4.451\n",
      "Epoch   0 Batch 1889/11516   train_loss = 9.501\n",
      "Epoch   0 Batch 1890/11516   train_loss = 6.355\n",
      "Epoch   0 Batch 1891/11516   train_loss = 6.482\n",
      "Epoch   0 Batch 1892/11516   train_loss = 8.315\n",
      "Epoch   0 Batch 1893/11516   train_loss = 7.937\n",
      "Epoch   0 Batch 1894/11516   train_loss = 5.521\n",
      "Epoch   0 Batch 1895/11516   train_loss = 7.140\n",
      "Epoch   0 Batch 1896/11516   train_loss = 3.780\n",
      "Epoch   0 Batch 1897/11516   train_loss = 9.970\n",
      "Epoch   0 Batch 1898/11516   train_loss = 8.291\n",
      "Epoch   0 Batch 1899/11516   train_loss = 8.119\n",
      "Epoch   0 Batch 1900/11516   train_loss = 5.025\n",
      "Epoch   0 Batch 1901/11516   train_loss = 9.005\n",
      "Epoch   0 Batch 1902/11516   train_loss = 3.806\n",
      "Epoch   0 Batch 1903/11516   train_loss = 7.708\n",
      "Epoch   0 Batch 1904/11516   train_loss = 6.778\n",
      "Epoch   0 Batch 1905/11516   train_loss = 8.729\n",
      "Epoch   0 Batch 1906/11516   train_loss = 7.329\n",
      "Epoch   0 Batch 1907/11516   train_loss = 7.178\n",
      "Epoch   0 Batch 1908/11516   train_loss = 8.383\n",
      "Epoch   0 Batch 1909/11516   train_loss = 3.524\n",
      "Epoch   0 Batch 1910/11516   train_loss = 10.947\n",
      "Epoch   0 Batch 1911/11516   train_loss = 10.099\n",
      "Epoch   0 Batch 1912/11516   train_loss = 7.582\n",
      "Epoch   0 Batch 1913/11516   train_loss = 5.768\n",
      "Epoch   0 Batch 1914/11516   train_loss = 8.014\n",
      "Epoch   0 Batch 1915/11516   train_loss = 6.568\n",
      "Epoch   0 Batch 1916/11516   train_loss = 6.268\n",
      "Epoch   0 Batch 1917/11516   train_loss = 7.780\n",
      "Epoch   0 Batch 1918/11516   train_loss = 5.942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1919/11516   train_loss = 4.777\n",
      "Epoch   0 Batch 1920/11516   train_loss = 7.314\n",
      "Epoch   0 Batch 1921/11516   train_loss = 6.679\n",
      "Epoch   0 Batch 1922/11516   train_loss = 3.245\n",
      "Epoch   0 Batch 1923/11516   train_loss = 6.751\n",
      "Epoch   0 Batch 1924/11516   train_loss = 6.657\n",
      "Epoch   0 Batch 1925/11516   train_loss = 4.295\n",
      "Epoch   0 Batch 1926/11516   train_loss = 4.725\n",
      "Epoch   0 Batch 1927/11516   train_loss = 7.649\n",
      "Epoch   0 Batch 1928/11516   train_loss = 5.948\n",
      "Epoch   0 Batch 1929/11516   train_loss = 6.904\n",
      "Epoch   0 Batch 1930/11516   train_loss = 4.709\n",
      "Epoch   0 Batch 1931/11516   train_loss = 6.952\n",
      "Epoch   0 Batch 1932/11516   train_loss = 8.214\n",
      "Epoch   0 Batch 1933/11516   train_loss = 10.020\n",
      "Epoch   0 Batch 1934/11516   train_loss = 4.748\n",
      "Epoch   0 Batch 1935/11516   train_loss = 4.734\n",
      "Epoch   0 Batch 1936/11516   train_loss = 5.048\n",
      "Epoch   0 Batch 1937/11516   train_loss = 6.635\n",
      "Epoch   0 Batch 1938/11516   train_loss = 6.250\n",
      "Epoch   0 Batch 1939/11516   train_loss = 5.478\n",
      "Epoch   0 Batch 1940/11516   train_loss = 7.984\n",
      "Epoch   0 Batch 1941/11516   train_loss = 3.481\n",
      "Epoch   0 Batch 1942/11516   train_loss = 8.165\n",
      "Epoch   0 Batch 1943/11516   train_loss = 8.027\n",
      "Epoch   0 Batch 1944/11516   train_loss = 8.097\n",
      "Epoch   0 Batch 1945/11516   train_loss = 7.684\n",
      "Epoch   0 Batch 1946/11516   train_loss = 6.472\n",
      "Epoch   0 Batch 1947/11516   train_loss = 6.938\n",
      "Epoch   0 Batch 1948/11516   train_loss = 7.924\n",
      "Epoch   0 Batch 1949/11516   train_loss = 5.447\n",
      "Epoch   0 Batch 1950/11516   train_loss = 7.822\n",
      "Epoch   0 Batch 1951/11516   train_loss = 6.296\n",
      "Epoch   0 Batch 1952/11516   train_loss = 8.493\n",
      "Epoch   0 Batch 1953/11516   train_loss = 6.770\n",
      "Epoch   0 Batch 1954/11516   train_loss = 4.741\n",
      "Epoch   0 Batch 1955/11516   train_loss = 6.654\n",
      "Epoch   0 Batch 1956/11516   train_loss = 5.257\n",
      "Epoch   0 Batch 1957/11516   train_loss = 5.283\n",
      "Epoch   0 Batch 1958/11516   train_loss = 6.562\n",
      "Epoch   0 Batch 1959/11516   train_loss = 5.424\n",
      "Epoch   0 Batch 1960/11516   train_loss = 5.061\n",
      "Epoch   0 Batch 1961/11516   train_loss = 8.701\n",
      "Epoch   0 Batch 1962/11516   train_loss = 4.912\n",
      "Epoch   0 Batch 1963/11516   train_loss = 4.269\n",
      "Epoch   0 Batch 1964/11516   train_loss = 6.085\n",
      "Epoch   0 Batch 1965/11516   train_loss = 7.794\n",
      "Epoch   0 Batch 1966/11516   train_loss = 5.077\n",
      "Epoch   0 Batch 1967/11516   train_loss = 5.692\n",
      "Epoch   0 Batch 1968/11516   train_loss = 5.456\n",
      "Epoch   0 Batch 1969/11516   train_loss = 4.332\n",
      "Epoch   0 Batch 1970/11516   train_loss = 7.437\n",
      "Epoch   0 Batch 1971/11516   train_loss = 6.216\n",
      "Epoch   0 Batch 1972/11516   train_loss = 4.165\n",
      "Epoch   0 Batch 1973/11516   train_loss = 9.152\n",
      "Epoch   0 Batch 1974/11516   train_loss = 7.970\n",
      "Epoch   0 Batch 1975/11516   train_loss = 4.272\n",
      "Epoch   0 Batch 1976/11516   train_loss = 3.626\n",
      "Epoch   0 Batch 1977/11516   train_loss = 6.660\n",
      "Epoch   0 Batch 1978/11516   train_loss = 6.053\n",
      "Epoch   0 Batch 1979/11516   train_loss = 7.171\n",
      "Epoch   0 Batch 1980/11516   train_loss = 6.261\n",
      "Epoch   0 Batch 1981/11516   train_loss = 6.638\n",
      "Epoch   0 Batch 1982/11516   train_loss = 7.276\n",
      "Epoch   0 Batch 1983/11516   train_loss = 5.081\n",
      "Epoch   0 Batch 1984/11516   train_loss = 3.557\n",
      "Epoch   0 Batch 1985/11516   train_loss = 5.620\n",
      "Epoch   0 Batch 1986/11516   train_loss = 5.937\n",
      "Epoch   0 Batch 1987/11516   train_loss = 4.778\n",
      "Epoch   0 Batch 1988/11516   train_loss = 5.424\n",
      "Epoch   0 Batch 1989/11516   train_loss = 6.135\n",
      "Epoch   0 Batch 1990/11516   train_loss = 7.852\n",
      "Epoch   0 Batch 1991/11516   train_loss = 7.413\n",
      "Epoch   0 Batch 1992/11516   train_loss = 8.814\n",
      "Epoch   0 Batch 1993/11516   train_loss = 8.216\n",
      "Epoch   0 Batch 1994/11516   train_loss = 8.684\n",
      "Epoch   0 Batch 1995/11516   train_loss = 4.403\n",
      "Epoch   0 Batch 1996/11516   train_loss = 9.121\n",
      "Epoch   0 Batch 1997/11516   train_loss = 4.131\n",
      "Epoch   0 Batch 1998/11516   train_loss = 7.456\n",
      "Epoch   0 Batch 1999/11516   train_loss = 5.300\n",
      "Epoch   0 Batch 2000/11516   train_loss = 8.376\n",
      "Epoch   0 Batch 2001/11516   train_loss = 8.175\n",
      "Epoch   0 Batch 2002/11516   train_loss = 4.028\n",
      "Epoch   0 Batch 2003/11516   train_loss = 9.744\n",
      "Epoch   0 Batch 2004/11516   train_loss = 4.318\n",
      "Epoch   0 Batch 2005/11516   train_loss = 11.523\n",
      "Epoch   0 Batch 2006/11516   train_loss = 8.238\n",
      "Epoch   0 Batch 2007/11516   train_loss = 5.839\n",
      "Epoch   0 Batch 2008/11516   train_loss = 6.016\n",
      "Epoch   0 Batch 2009/11516   train_loss = 7.539\n",
      "Epoch   0 Batch 2010/11516   train_loss = 5.851\n",
      "Epoch   0 Batch 2011/11516   train_loss = 8.515\n",
      "Epoch   0 Batch 2012/11516   train_loss = 4.844\n",
      "Epoch   0 Batch 2013/11516   train_loss = 5.491\n",
      "Epoch   0 Batch 2014/11516   train_loss = 5.759\n",
      "Epoch   0 Batch 2015/11516   train_loss = 7.002\n",
      "Epoch   0 Batch 2016/11516   train_loss = 5.927\n",
      "Epoch   0 Batch 2017/11516   train_loss = 10.001\n",
      "Epoch   0 Batch 2018/11516   train_loss = 7.577\n",
      "Epoch   0 Batch 2019/11516   train_loss = 5.651\n",
      "Epoch   0 Batch 2020/11516   train_loss = 5.130\n",
      "Epoch   0 Batch 2021/11516   train_loss = 5.881\n",
      "Epoch   0 Batch 2022/11516   train_loss = 6.400\n",
      "Epoch   0 Batch 2023/11516   train_loss = 6.905\n",
      "Epoch   0 Batch 2024/11516   train_loss = 7.376\n",
      "Epoch   0 Batch 2025/11516   train_loss = 6.563\n",
      "Epoch   0 Batch 2026/11516   train_loss = 6.319\n",
      "Epoch   0 Batch 2027/11516   train_loss = 5.674\n",
      "Epoch   0 Batch 2028/11516   train_loss = 7.296\n",
      "Epoch   0 Batch 2029/11516   train_loss = 6.249\n",
      "Epoch   0 Batch 2030/11516   train_loss = 7.466\n",
      "Epoch   0 Batch 2031/11516   train_loss = 5.705\n",
      "Epoch   0 Batch 2032/11516   train_loss = 4.535\n",
      "Epoch   0 Batch 2033/11516   train_loss = 6.570\n",
      "Epoch   0 Batch 2034/11516   train_loss = 5.868\n",
      "Epoch   0 Batch 2035/11516   train_loss = 7.624\n",
      "Epoch   0 Batch 2036/11516   train_loss = 7.486\n",
      "Epoch   0 Batch 2037/11516   train_loss = 4.922\n",
      "Epoch   0 Batch 2038/11516   train_loss = 5.644\n",
      "Epoch   0 Batch 2039/11516   train_loss = 5.891\n",
      "Epoch   0 Batch 2040/11516   train_loss = 4.624\n",
      "Epoch   0 Batch 2041/11516   train_loss = 6.363\n",
      "Epoch   0 Batch 2042/11516   train_loss = 3.619\n",
      "Epoch   0 Batch 2043/11516   train_loss = 5.543\n",
      "Epoch   0 Batch 2044/11516   train_loss = 4.054\n",
      "Epoch   0 Batch 2045/11516   train_loss = 4.390\n",
      "Epoch   0 Batch 2046/11516   train_loss = 4.761\n",
      "Epoch   0 Batch 2047/11516   train_loss = 6.233\n",
      "Epoch   0 Batch 2048/11516   train_loss = 7.944\n",
      "Epoch   0 Batch 2049/11516   train_loss = 7.815\n",
      "Epoch   0 Batch 2050/11516   train_loss = 5.916\n",
      "Epoch   0 Batch 2051/11516   train_loss = 7.299\n",
      "Epoch   0 Batch 2052/11516   train_loss = 4.866\n",
      "Epoch   0 Batch 2053/11516   train_loss = 5.965\n",
      "Epoch   0 Batch 2054/11516   train_loss = 6.943\n",
      "Epoch   0 Batch 2055/11516   train_loss = 3.580\n",
      "Epoch   0 Batch 2056/11516   train_loss = 7.832\n",
      "Epoch   0 Batch 2057/11516   train_loss = 5.127\n",
      "Epoch   0 Batch 2058/11516   train_loss = 5.749\n",
      "Epoch   0 Batch 2059/11516   train_loss = 6.569\n",
      "Epoch   0 Batch 2060/11516   train_loss = 5.621\n",
      "Epoch   0 Batch 2061/11516   train_loss = 5.559\n",
      "Epoch   0 Batch 2062/11516   train_loss = 9.901\n",
      "Epoch   0 Batch 2063/11516   train_loss = 6.903\n",
      "Epoch   0 Batch 2064/11516   train_loss = 2.769\n",
      "Epoch   0 Batch 2065/11516   train_loss = 4.422\n",
      "Epoch   0 Batch 2066/11516   train_loss = 8.304\n",
      "Epoch   0 Batch 2067/11516   train_loss = 7.302\n",
      "Epoch   0 Batch 2068/11516   train_loss = 8.580\n",
      "Epoch   0 Batch 2069/11516   train_loss = 7.558\n",
      "Epoch   0 Batch 2070/11516   train_loss = 10.750\n",
      "Epoch   0 Batch 2071/11516   train_loss = 4.461\n",
      "Epoch   0 Batch 2072/11516   train_loss = 5.174\n",
      "Epoch   0 Batch 2073/11516   train_loss = 3.776\n",
      "Epoch   0 Batch 2074/11516   train_loss = 7.021\n",
      "Epoch   0 Batch 2075/11516   train_loss = 5.960\n",
      "Epoch   0 Batch 2076/11516   train_loss = 6.982\n",
      "Epoch   0 Batch 2077/11516   train_loss = 7.527\n",
      "Epoch   0 Batch 2078/11516   train_loss = 7.199\n",
      "Epoch   0 Batch 2079/11516   train_loss = 4.946\n",
      "Epoch   0 Batch 2080/11516   train_loss = 6.619\n",
      "Epoch   0 Batch 2081/11516   train_loss = 5.543\n",
      "Epoch   0 Batch 2082/11516   train_loss = 7.628\n",
      "Epoch   0 Batch 2083/11516   train_loss = 8.424\n",
      "Epoch   0 Batch 2084/11516   train_loss = 3.480\n",
      "Epoch   0 Batch 2085/11516   train_loss = 6.025\n",
      "Epoch   0 Batch 2086/11516   train_loss = 5.617\n",
      "Epoch   0 Batch 2087/11516   train_loss = 11.585\n",
      "Epoch   0 Batch 2088/11516   train_loss = 5.516\n",
      "Epoch   0 Batch 2089/11516   train_loss = 9.020\n",
      "Epoch   0 Batch 2090/11516   train_loss = 7.375\n",
      "Epoch   0 Batch 2091/11516   train_loss = 4.474\n",
      "Epoch   0 Batch 2092/11516   train_loss = 6.372\n",
      "Epoch   0 Batch 2093/11516   train_loss = 4.284\n",
      "Epoch   0 Batch 2094/11516   train_loss = 6.361\n",
      "Epoch   0 Batch 2095/11516   train_loss = 5.056\n",
      "Epoch   0 Batch 2096/11516   train_loss = 6.069\n",
      "Epoch   0 Batch 2097/11516   train_loss = 8.629\n",
      "Epoch   0 Batch 2098/11516   train_loss = 7.197\n",
      "Epoch   0 Batch 2099/11516   train_loss = 8.630\n",
      "Epoch   0 Batch 2100/11516   train_loss = 9.684\n",
      "Epoch   0 Batch 2101/11516   train_loss = 6.350\n",
      "Epoch   0 Batch 2102/11516   train_loss = 13.405\n",
      "Epoch   0 Batch 2103/11516   train_loss = 8.312\n",
      "Epoch   0 Batch 2104/11516   train_loss = 4.958\n",
      "Epoch   0 Batch 2105/11516   train_loss = 4.937\n",
      "Epoch   0 Batch 2106/11516   train_loss = 7.250\n",
      "Epoch   0 Batch 2107/11516   train_loss = 9.112\n",
      "Epoch   0 Batch 2108/11516   train_loss = 9.055\n",
      "Epoch   0 Batch 2109/11516   train_loss = 6.387\n",
      "Epoch   0 Batch 2110/11516   train_loss = 6.710\n",
      "Epoch   0 Batch 2111/11516   train_loss = 9.305\n",
      "Epoch   0 Batch 2112/11516   train_loss = 7.063\n",
      "Epoch   0 Batch 2113/11516   train_loss = 7.034\n",
      "Epoch   0 Batch 2114/11516   train_loss = 5.597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 2115/11516   train_loss = 7.811\n",
      "Epoch   0 Batch 2116/11516   train_loss = 6.703\n",
      "Epoch   0 Batch 2117/11516   train_loss = 7.557\n",
      "Epoch   0 Batch 2118/11516   train_loss = 5.232\n",
      "Epoch   0 Batch 2119/11516   train_loss = 6.754\n",
      "Epoch   0 Batch 2120/11516   train_loss = 5.417\n",
      "Epoch   0 Batch 2121/11516   train_loss = 8.995\n",
      "Epoch   0 Batch 2122/11516   train_loss = 5.214\n",
      "Epoch   0 Batch 2123/11516   train_loss = 6.645\n",
      "Epoch   0 Batch 2124/11516   train_loss = 8.917\n",
      "Epoch   0 Batch 2125/11516   train_loss = 4.804\n",
      "Epoch   0 Batch 2126/11516   train_loss = 4.797\n",
      "Epoch   0 Batch 2127/11516   train_loss = 7.706\n",
      "Epoch   0 Batch 2128/11516   train_loss = 6.315\n",
      "Epoch   0 Batch 2129/11516   train_loss = 2.963\n",
      "Epoch   0 Batch 2130/11516   train_loss = 9.153\n",
      "Epoch   0 Batch 2131/11516   train_loss = 5.022\n",
      "Epoch   0 Batch 2132/11516   train_loss = 6.727\n",
      "Epoch   0 Batch 2133/11516   train_loss = 5.339\n",
      "Epoch   0 Batch 2134/11516   train_loss = 5.406\n",
      "Epoch   0 Batch 2135/11516   train_loss = 6.503\n",
      "Epoch   0 Batch 2136/11516   train_loss = 7.471\n",
      "Epoch   0 Batch 2137/11516   train_loss = 5.791\n",
      "Epoch   0 Batch 2138/11516   train_loss = 5.302\n",
      "Epoch   0 Batch 2139/11516   train_loss = 4.095\n",
      "Epoch   0 Batch 2140/11516   train_loss = 6.733\n",
      "Epoch   0 Batch 2141/11516   train_loss = 5.207\n",
      "Epoch   0 Batch 2142/11516   train_loss = 5.518\n",
      "Epoch   0 Batch 2143/11516   train_loss = 4.193\n",
      "Epoch   0 Batch 2144/11516   train_loss = 4.093\n",
      "Epoch   0 Batch 2145/11516   train_loss = 7.449\n",
      "Epoch   0 Batch 2146/11516   train_loss = 6.815\n",
      "Epoch   0 Batch 2147/11516   train_loss = 6.039\n",
      "Epoch   0 Batch 2148/11516   train_loss = 6.493\n",
      "Epoch   0 Batch 2149/11516   train_loss = 9.059\n",
      "Epoch   0 Batch 2150/11516   train_loss = 6.758\n",
      "Epoch   0 Batch 2151/11516   train_loss = 4.527\n",
      "Epoch   0 Batch 2152/11516   train_loss = 6.011\n",
      "Epoch   0 Batch 2153/11516   train_loss = 8.835\n",
      "Epoch   0 Batch 2154/11516   train_loss = 4.800\n",
      "Epoch   0 Batch 2155/11516   train_loss = 8.331\n",
      "Epoch   0 Batch 2156/11516   train_loss = 5.609\n",
      "Epoch   0 Batch 2157/11516   train_loss = 5.411\n",
      "Epoch   0 Batch 2158/11516   train_loss = 7.715\n",
      "Epoch   0 Batch 2159/11516   train_loss = 3.840\n",
      "Epoch   0 Batch 2160/11516   train_loss = 3.927\n",
      "Epoch   0 Batch 2161/11516   train_loss = 5.794\n",
      "Epoch   0 Batch 2162/11516   train_loss = 4.397\n",
      "Epoch   0 Batch 2163/11516   train_loss = 4.228\n",
      "Epoch   0 Batch 2164/11516   train_loss = 5.723\n",
      "Epoch   0 Batch 2165/11516   train_loss = 5.690\n",
      "Epoch   0 Batch 2166/11516   train_loss = 5.250\n",
      "Epoch   0 Batch 2167/11516   train_loss = 7.848\n",
      "Epoch   0 Batch 2168/11516   train_loss = 7.908\n",
      "Epoch   0 Batch 2169/11516   train_loss = 5.726\n",
      "Epoch   0 Batch 2170/11516   train_loss = 4.791\n",
      "Epoch   0 Batch 2171/11516   train_loss = 5.145\n",
      "Epoch   0 Batch 2172/11516   train_loss = 7.107\n",
      "Epoch   0 Batch 2173/11516   train_loss = 4.959\n",
      "Epoch   0 Batch 2174/11516   train_loss = 5.204\n",
      "Epoch   0 Batch 2175/11516   train_loss = 7.502\n",
      "Epoch   0 Batch 2176/11516   train_loss = 8.155\n",
      "Epoch   0 Batch 2177/11516   train_loss = 6.046\n",
      "Epoch   0 Batch 2178/11516   train_loss = 5.308\n",
      "Epoch   0 Batch 2179/11516   train_loss = 7.825\n",
      "Epoch   0 Batch 2180/11516   train_loss = 5.337\n",
      "Epoch   0 Batch 2181/11516   train_loss = 5.067\n",
      "Epoch   0 Batch 2182/11516   train_loss = 5.743\n",
      "Epoch   0 Batch 2183/11516   train_loss = 5.814\n",
      "Epoch   0 Batch 2184/11516   train_loss = 6.730\n",
      "Epoch   0 Batch 2185/11516   train_loss = 7.672\n",
      "Epoch   0 Batch 2186/11516   train_loss = 5.095\n",
      "Epoch   0 Batch 2187/11516   train_loss = 8.425\n",
      "Epoch   0 Batch 2188/11516   train_loss = 7.098\n",
      "Epoch   0 Batch 2189/11516   train_loss = 3.506\n",
      "Epoch   0 Batch 2190/11516   train_loss = 6.091\n",
      "Epoch   0 Batch 2191/11516   train_loss = 7.928\n",
      "Epoch   0 Batch 2192/11516   train_loss = 9.241\n",
      "Epoch   0 Batch 2193/11516   train_loss = 6.672\n",
      "Epoch   0 Batch 2194/11516   train_loss = 8.118\n",
      "Epoch   0 Batch 2195/11516   train_loss = 5.095\n",
      "Epoch   0 Batch 2196/11516   train_loss = 7.109\n",
      "Epoch   0 Batch 2197/11516   train_loss = 6.497\n",
      "Epoch   0 Batch 2198/11516   train_loss = 9.171\n",
      "Epoch   0 Batch 2199/11516   train_loss = 6.157\n",
      "Epoch   0 Batch 2200/11516   train_loss = 4.459\n",
      "Epoch   0 Batch 2201/11516   train_loss = 5.546\n",
      "Epoch   0 Batch 2202/11516   train_loss = 7.650\n",
      "Epoch   0 Batch 2203/11516   train_loss = 5.566\n",
      "Epoch   0 Batch 2204/11516   train_loss = 8.365\n",
      "Epoch   0 Batch 2205/11516   train_loss = 5.688\n",
      "Epoch   0 Batch 2206/11516   train_loss = 5.577\n",
      "Epoch   0 Batch 2207/11516   train_loss = 5.992\n",
      "Epoch   0 Batch 2208/11516   train_loss = 7.267\n",
      "Epoch   0 Batch 2209/11516   train_loss = 5.594\n",
      "Epoch   0 Batch 2210/11516   train_loss = 8.820\n",
      "Epoch   0 Batch 2211/11516   train_loss = 8.659\n",
      "Epoch   0 Batch 2212/11516   train_loss = 5.449\n",
      "Epoch   0 Batch 2213/11516   train_loss = 6.057\n",
      "Epoch   0 Batch 2214/11516   train_loss = 6.516\n",
      "Epoch   0 Batch 2215/11516   train_loss = 5.328\n",
      "Epoch   0 Batch 2216/11516   train_loss = 4.207\n",
      "Epoch   0 Batch 2217/11516   train_loss = 7.092\n",
      "Epoch   0 Batch 2218/11516   train_loss = 8.403\n",
      "Epoch   0 Batch 2219/11516   train_loss = 8.385\n",
      "Epoch   0 Batch 2220/11516   train_loss = 7.572\n",
      "Epoch   0 Batch 2221/11516   train_loss = 6.982\n",
      "Epoch   0 Batch 2222/11516   train_loss = 3.031\n",
      "Epoch   0 Batch 2223/11516   train_loss = 4.183\n",
      "Epoch   0 Batch 2224/11516   train_loss = 4.704\n",
      "Epoch   0 Batch 2225/11516   train_loss = 4.972\n",
      "Epoch   0 Batch 2226/11516   train_loss = 7.507\n",
      "Epoch   0 Batch 2227/11516   train_loss = 4.972\n",
      "Epoch   0 Batch 2228/11516   train_loss = 6.864\n",
      "Epoch   0 Batch 2229/11516   train_loss = 5.728\n",
      "Epoch   0 Batch 2230/11516   train_loss = 6.054\n",
      "Epoch   0 Batch 2231/11516   train_loss = 2.662\n",
      "Epoch   0 Batch 2232/11516   train_loss = 3.973\n",
      "Epoch   0 Batch 2233/11516   train_loss = 5.653\n",
      "Epoch   0 Batch 2234/11516   train_loss = 4.624\n",
      "Epoch   0 Batch 2235/11516   train_loss = 8.045\n",
      "Epoch   0 Batch 2236/11516   train_loss = 2.588\n",
      "Epoch   0 Batch 2237/11516   train_loss = 6.818\n",
      "Epoch   0 Batch 2238/11516   train_loss = 5.925\n",
      "Epoch   0 Batch 2239/11516   train_loss = 4.682\n",
      "Epoch   0 Batch 2240/11516   train_loss = 7.688\n",
      "Epoch   0 Batch 2241/11516   train_loss = 7.609\n",
      "Epoch   0 Batch 2242/11516   train_loss = 3.693\n",
      "Epoch   0 Batch 2243/11516   train_loss = 6.411\n",
      "Epoch   0 Batch 2244/11516   train_loss = 5.078\n",
      "Epoch   0 Batch 2245/11516   train_loss = 5.541\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received a label value of 6776 which is outside the valid range of [0, 6776).  Label values: 3513 6776 5425 1213 2308 1748\n\t [[Node: sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sequence_loss/Reshape, sequence_loss/Reshape_1)]]\n\nCaused by op 'sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/george/dlearning/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-43-a2d9a7091ca5>\", line 21, in <module>\n    tf.ones([input_data_shape[0], input_data_shape[1]]))\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 76, in sequence_loss\n    labels=targets, logits=probs_flat)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1742, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2418, in _sparse_softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Received a label value of 6776 which is outside the valid range of [0, 6776).  Label values: 3513 6776 5425 1213 2308 1748\n\t [[Node: sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sequence_loss/Reshape, sequence_loss/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 6776 which is outside the valid range of [0, 6776).  Label values: 3513 6776 5425 1213 2308 1748\n\t [[Node: sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sequence_loss/Reshape, sequence_loss/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-396c263e7e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 lr: learning_rate}\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Show every <show_every_n_batches> batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/dlearning/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received a label value of 6776 which is outside the valid range of [0, 6776).  Label values: 3513 6776 5425 1213 2308 1748\n\t [[Node: sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sequence_loss/Reshape, sequence_loss/Reshape_1)]]\n\nCaused by op 'sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/george/dlearning/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-43-a2d9a7091ca5>\", line 21, in <module>\n    tf.ones([input_data_shape[0], input_data_shape[1]]))\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/loss.py\", line 76, in sequence_loss\n    labels=targets, logits=probs_flat)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 1742, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2418, in _sparse_softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/george/dlearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Received a label value of 6776 which is outside the valid range of [0, 6776).  Label values: 3513 6776 5425 1213 2308 1748\n\t [[Node: sequence_loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](sequence_loss/Reshape, sequence_loss/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
